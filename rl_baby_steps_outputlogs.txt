#Run 1 - Random actions

Episode 1, score=15.0
Episode 2, score=24.0
Episode 3, score=24.0
Episode 4, score=18.0
Episode 5, score=13.0
Episode 6, score=14.0
Episode 7, score=20.0
Episode 8, score=13.0
Episode 9, score=33.0
Episode 10, score=35.0

-----------------------------------------------------------------------------------
###################################################################################
-----------------------------------------------------------------------------------
Run 2
Training for 10000 steps ...
   10/10000: episode: 1, duration: 0.042s, episode steps:  10, steps per second: 236, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: --, mae: --, mean_q: --
   25/10000: episode: 2, duration: 0.174s, episode steps:  15, steps per second:  86, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.541401, mae: 0.575754, mean_q: 0.124254
   36/10000: episode: 3, duration: 0.018s, episode steps:  11, steps per second: 610, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.510082, mae: 0.557423, mean_q: 0.192902
   46/10000: episode: 4, duration: 0.031s, episode steps:  10, steps per second: 326, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.426661, mae: 0.529925, mean_q: 0.270106
   59/10000: episode: 5, duration: 0.030s, episode steps:  13, steps per second: 431, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.387958, mae: 0.552585, mean_q: 0.393537
   75/10000: episode: 6, duration: 0.024s, episode steps:  16, steps per second: 660, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.188 [0.000, 1.000],  loss: 0.315785, mae: 0.564806, mean_q: 0.532089
   86/10000: episode: 7, duration: 0.017s, episode steps:  11, steps per second: 633, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.237316, mae: 0.570200, mean_q: 0.708419
  128/10000: episode: 8, duration: 0.062s, episode steps:  42, steps per second: 678, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.174496, mae: 0.670777, mean_q: 0.987250
  195/10000: episode: 9, duration: 0.098s, episode steps:  67, steps per second: 685, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.110385, mae: 0.842089, mean_q: 1.394182
  206/10000: episode: 10, duration: 0.017s, episode steps:  11, steps per second: 634, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.084145, mae: 0.960348, mean_q: 1.765874
  227/10000: episode: 11, duration: 0.031s, episode steps:  21, steps per second: 669, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.097169, mae: 1.023536, mean_q: 1.913382
  270/10000: episode: 12, duration: 0.062s, episode steps:  43, steps per second: 696, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.651 [0.000, 1.000],  loss: 0.065232, mae: 1.101017, mean_q: 2.148717
  293/10000: episode: 13, duration: 0.033s, episode steps:  23, steps per second: 690, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.066754, mae: 1.215266, mean_q: 2.377074
  315/10000: episode: 14, duration: 0.032s, episode steps:  22, steps per second: 695, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.124222, mae: 1.364251, mean_q: 2.678797
  341/10000: episode: 15, duration: 0.037s, episode steps:  26, steps per second: 703, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 0.134807, mae: 1.449741, mean_q: 2.795140
  357/10000: episode: 16, duration: 0.024s, episode steps:  16, steps per second: 679, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.061533, mae: 1.499655, mean_q: 2.980739
  386/10000: episode: 17, duration: 0.041s, episode steps:  29, steps per second: 700, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.135575, mae: 1.629689, mean_q: 3.206405
  414/10000: episode: 18, duration: 0.040s, episode steps:  28, steps per second: 706, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.100968, mae: 1.743940, mean_q: 3.429747
  432/10000: episode: 19, duration: 0.029s, episode steps:  18, steps per second: 618, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.178861, mae: 1.852827, mean_q: 3.602498
  442/10000: episode: 20, duration: 0.016s, episode steps:  10, steps per second: 633, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.174393, mae: 1.908911, mean_q: 3.701096
  513/10000: episode: 21, duration: 0.102s, episode steps:  71, steps per second: 696, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.606 [0.000, 1.000],  loss: 0.205148, mae: 2.081444, mean_q: 4.018554
  524/10000: episode: 22, duration: 0.017s, episode steps:  11, steps per second: 644, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.253732, mae: 2.261609, mean_q: 4.377188
  536/10000: episode: 23, duration: 0.018s, episode steps:  12, steps per second: 657, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.194455, mae: 2.299708, mean_q: 4.482961
  554/10000: episode: 24, duration: 0.027s, episode steps:  18, steps per second: 675, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.241685, mae: 2.356759, mean_q: 4.604324
  586/10000: episode: 25, duration: 0.047s, episode steps:  32, steps per second: 686, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.279666, mae: 2.483194, mean_q: 4.770270
  659/10000: episode: 26, duration: 0.104s, episode steps:  73, steps per second: 705, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.273558, mae: 2.698374, mean_q: 5.230793
  670/10000: episode: 27, duration: 0.017s, episode steps:  11, steps per second: 644, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.324457, mae: 2.891985, mean_q: 5.629799
  679/10000: episode: 28, duration: 0.014s, episode steps:   9, steps per second: 634, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.408542, mae: 2.940808, mean_q: 5.601155
  702/10000: episode: 29, duration: 0.033s, episode steps:  23, steps per second: 690, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.382699, mae: 2.985716, mean_q: 5.771744
  713/10000: episode: 30, duration: 0.017s, episode steps:  11, steps per second: 665, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.429561, mae: 3.113783, mean_q: 5.928917
  741/10000: episode: 31, duration: 0.040s, episode steps:  28, steps per second: 704, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.415924, mae: 3.139836, mean_q: 6.033495
  768/10000: episode: 32, duration: 0.038s, episode steps:  27, steps per second: 702, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.436081, mae: 3.264763, mean_q: 6.221168
  785/10000: episode: 33, duration: 0.025s, episode steps:  17, steps per second: 685, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.471145, mae: 3.333996, mean_q: 6.334532
  803/10000: episode: 34, duration: 0.026s, episode steps:  18, steps per second: 682, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.426806, mae: 3.397216, mean_q: 6.533540
  865/10000: episode: 35, duration: 0.088s, episode steps:  62, steps per second: 707, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.429466, mae: 3.546180, mean_q: 6.783129
  882/10000: episode: 36, duration: 0.025s, episode steps:  17, steps per second: 674, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.558947, mae: 3.724086, mean_q: 7.082577
  899/10000: episode: 37, duration: 0.025s, episode steps:  17, steps per second: 683, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.524196, mae: 3.766965, mean_q: 7.232659
  914/10000: episode: 38, duration: 0.022s, episode steps:  15, steps per second: 683, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.595411, mae: 3.807205, mean_q: 7.212662
  943/10000: episode: 39, duration: 0.041s, episode steps:  29, steps per second: 709, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.488069, mae: 3.887258, mean_q: 7.430610
  974/10000: episode: 40, duration: 0.044s, episode steps:  31, steps per second: 711, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 0.641952, mae: 4.031791, mean_q: 7.736495
  990/10000: episode: 41, duration: 0.023s, episode steps:  16, steps per second: 682, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.466292, mae: 4.076171, mean_q: 7.926237
 1009/10000: episode: 42, duration: 0.027s, episode steps:  19, steps per second: 699, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.577858, mae: 4.169096, mean_q: 8.058824
 1054/10000: episode: 43, duration: 0.062s, episode steps:  45, steps per second: 723, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.675623, mae: 4.279622, mean_q: 8.170765
 1071/10000: episode: 44, duration: 0.025s, episode steps:  17, steps per second: 693, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.665826, mae: 4.364753, mean_q: 8.340804
 1085/10000: episode: 45, duration: 0.020s, episode steps:  14, steps per second: 683, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.952939, mae: 4.423165, mean_q: 8.334531
 1144/10000: episode: 46, duration: 0.081s, episode steps:  59, steps per second: 728, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.638217, mae: 4.571093, mean_q: 8.823207
 1162/10000: episode: 47, duration: 0.026s, episode steps:  18, steps per second: 686, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.609267, mae: 4.713482, mean_q: 9.141719
 1176/10000: episode: 48, duration: 0.020s, episode steps:  14, steps per second: 686, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.829275, mae: 4.747272, mean_q: 9.069180
 1201/10000: episode: 49, duration: 0.035s, episode steps:  25, steps per second: 708, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.709477, mae: 4.876617, mean_q: 9.417481
 1261/10000: episode: 50, duration: 0.083s, episode steps:  60, steps per second: 723, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.627803, mae: 5.026846, mean_q: 9.800150
 1318/10000: episode: 51, duration: 0.078s, episode steps:  57, steps per second: 727, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.753056, mae: 5.238380, mean_q: 10.232036
 1362/10000: episode: 52, duration: 0.061s, episode steps:  44, steps per second: 719, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.386 [0.000, 1.000],  loss: 0.802153, mae: 5.441518, mean_q: 10.596675
 1380/10000: episode: 53, duration: 0.026s, episode steps:  18, steps per second: 694, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.299074, mae: 5.501449, mean_q: 10.501891
 1441/10000: episode: 54, duration: 0.084s, episode steps:  61, steps per second: 727, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.855318, mae: 5.714109, mean_q: 11.124003
 1473/10000: episode: 55, duration: 0.045s, episode steps:  32, steps per second: 713, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.770002, mae: 5.820418, mean_q: 11.424055
 1539/10000: episode: 56, duration: 0.091s, episode steps:  66, steps per second: 729, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.847033, mae: 6.031555, mean_q: 11.845492
 1551/10000: episode: 57, duration: 0.018s, episode steps:  12, steps per second: 684, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.993792, mae: 6.243186, mean_q: 12.263389
 1592/10000: episode: 58, duration: 0.057s, episode steps:  41, steps per second: 717, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 1.195403, mae: 6.271921, mean_q: 12.228886
 1611/10000: episode: 59, duration: 0.027s, episode steps:  19, steps per second: 698, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.991301, mae: 6.333229, mean_q: 12.416513
 1627/10000: episode: 60, duration: 0.023s, episode steps:  16, steps per second: 701, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.838157, mae: 6.386878, mean_q: 12.505041
 1737/10000: episode: 61, duration: 0.153s, episode steps: 110, steps per second: 721, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 1.032504, mae: 6.744028, mean_q: 13.269953
 1816/10000: episode: 62, duration: 0.112s, episode steps:  79, steps per second: 704, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.305223, mae: 7.027132, mean_q: 13.801242
 1900/10000: episode: 63, duration: 0.116s, episode steps:  84, steps per second: 722, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.464309, mae: 7.342833, mean_q: 14.447468
 1937/10000: episode: 64, duration: 0.053s, episode steps:  37, steps per second: 693, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.914628, mae: 7.621375, mean_q: 15.054819
 2140/10000: episode: 65, duration: 0.278s, episode steps: 203, steps per second: 730, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.189690, mae: 8.074594, mean_q: 16.032156
 2261/10000: episode: 66, duration: 0.166s, episode steps: 121, steps per second: 730, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.276762, mae: 8.673528, mean_q: 17.271770
 2285/10000: episode: 67, duration: 0.034s, episode steps:  24, steps per second: 708, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.489749, mae: 8.903174, mean_q: 17.697294
 2454/10000: episode: 68, duration: 0.230s, episode steps: 169, steps per second: 735, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.360788, mae: 9.261448, mean_q: 18.424606
 2613/10000: episode: 69, duration: 0.216s, episode steps: 159, steps per second: 736, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.756471, mae: 9.893058, mean_q: 19.695927
 2773/10000: episode: 70, duration: 0.219s, episode steps: 160, steps per second: 730, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.310347, mae: 10.495322, mean_q: 21.044559
 2899/10000: episode: 71, duration: 0.174s, episode steps: 126, steps per second: 725, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.803750, mae: 11.013766, mean_q: 22.042030
 3050/10000: episode: 72, duration: 0.206s, episode steps: 151, steps per second: 734, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.689919, mae: 11.496187, mean_q: 23.002733
 3247/10000: episode: 73, duration: 0.267s, episode steps: 197, steps per second: 738, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 1.880868, mae: 12.190025, mean_q: 24.499521
 3462/10000: episode: 74, duration: 0.296s, episode steps: 215, steps per second: 727, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.010069, mae: 13.014342, mean_q: 26.247375
 3628/10000: episode: 75, duration: 0.229s, episode steps: 166, steps per second: 726, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.401343, mae: 13.692188, mean_q: 27.555582
 3865/10000: episode: 76, duration: 0.327s, episode steps: 237, steps per second: 726, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.284389, mae: 14.404552, mean_q: 29.138885
 4032/10000: episode: 77, duration: 0.271s, episode steps: 167, steps per second: 616, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.991604, mae: 15.239760, mean_q: 30.941692
 4212/10000: episode: 78, duration: 0.253s, episode steps: 180, steps per second: 712, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.050869, mae: 15.901704, mean_q: 32.156868
 4382/10000: episode: 79, duration: 0.239s, episode steps: 170, steps per second: 710, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.790934, mae: 16.588026, mean_q: 33.715809
 4576/10000: episode: 80, duration: 0.266s, episode steps: 194, steps per second: 728, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.732420, mae: 17.282572, mean_q: 35.068111
 4796/10000: episode: 81, duration: 0.306s, episode steps: 220, steps per second: 719, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.234058, mae: 18.135603, mean_q: 36.925556
 5006/10000: episode: 82, duration: 0.293s, episode steps: 210, steps per second: 718, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.060503, mae: 19.045732, mean_q: 38.846615
 5206/10000: episode: 83, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 3.167404, mae: 19.898619, mean_q: 40.441250
 5407/10000: episode: 84, duration: 0.281s, episode steps: 201, steps per second: 716, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.395657, mae: 20.717484, mean_q: 42.122181
 5582/10000: episode: 85, duration: 0.240s, episode steps: 175, steps per second: 729, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.656960, mae: 21.472599, mean_q: 43.558392
 5785/10000: episode: 86, duration: 0.278s, episode steps: 203, steps per second: 731, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.911602, mae: 22.133438, mean_q: 44.930813
 6271/10000: episode: 87, duration: 0.668s, episode steps: 486, steps per second: 727, episode reward: 486.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.542707, mae: 23.359447, mean_q: 47.442074
 6480/10000: episode: 88, duration: 0.290s, episode steps: 209, steps per second: 720, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.041703, mae: 24.512800, mean_q: 49.782501
 6692/10000: episode: 89, duration: 0.292s, episode steps: 212, steps per second: 726, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.648940, mae: 25.295357, mean_q: 51.415501
 6893/10000: episode: 90, duration: 0.285s, episode steps: 201, steps per second: 706, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.383620, mae: 26.107136, mean_q: 53.032383
 7085/10000: episode: 91, duration: 0.271s, episode steps: 192, steps per second: 709, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.645577, mae: 26.514837, mean_q: 53.840290
 7297/10000: episode: 92, duration: 0.301s, episode steps: 212, steps per second: 705, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.974399, mae: 27.142048, mean_q: 55.021820
 7501/10000: episode: 93, duration: 0.288s, episode steps: 204, steps per second: 709, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.906524, mae: 27.878231, mean_q: 56.565804
 7676/10000: episode: 94, duration: 0.247s, episode steps: 175, steps per second: 710, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.104782, mae: 28.237299, mean_q: 57.352768
 7933/10000: episode: 95, duration: 0.364s, episode steps: 257, steps per second: 706, episode reward: 257.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.380011, mae: 28.945087, mean_q: 58.836628
 8122/10000: episode: 96, duration: 0.271s, episode steps: 189, steps per second: 698, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.015730, mae: 29.648540, mean_q: 60.212353
 8300/10000: episode: 97, duration: 0.255s, episode steps: 178, steps per second: 698, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 4.790098, mae: 30.139498, mean_q: 61.161045
 8637/10000: episode: 98, duration: 0.473s, episode steps: 337, steps per second: 712, episode reward: 337.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 4.678094, mae: 30.862923, mean_q: 62.531979
 8809/10000: episode: 99, duration: 0.241s, episode steps: 172, steps per second: 714, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.222992, mae: 31.620075, mean_q: 64.145721
 9050/10000: episode: 100, duration: 0.364s, episode steps: 241, steps per second: 662, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.486578, mae: 32.249725, mean_q: 65.474495
 9250/10000: episode: 101, duration: 0.277s, episode steps: 200, steps per second: 722, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 5.387255, mae: 32.620548, mean_q: 66.005859
 9465/10000: episode: 102, duration: 0.294s, episode steps: 215, steps per second: 730, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.038850, mae: 33.291191, mean_q: 67.417992
 9668/10000: episode: 103, duration: 0.278s, episode steps: 203, steps per second: 731, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.495684, mae: 33.512306, mean_q: 67.896889
 9870/10000: episode: 104, duration: 0.277s, episode steps: 202, steps per second: 729, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 4.792703, mae: 34.142387, mean_q: 69.151794
done, took 14.252 seconds
Testing for 10 episodes ...
Episode 1: reward: 307.000, steps: 307
Episode 2: reward: 164.000, steps: 164
Episode 3: reward: 184.000, steps: 184
Episode 4: reward: 186.000, steps: 186
Episode 5: reward: 173.000, steps: 173
Episode 6: reward: 164.000, steps: 164
Episode 7: reward: 209.000, steps: 209
Episode 8: reward: 173.000, steps: 173
Episode 9: reward: 239.000, steps: 239
Episode 10: reward: 197.000, steps: 197
[307.0, 164.0, 184.0, 186.0, 173.0, 164.0, 209.0, 173.0, 239.0, 197.0]
Training for 10000 steps ...
   14/10000: episode: 1, duration: 0.187s, episode steps:  14, steps per second:  75, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.507427, mae: 0.514259, mean_q: 0.027576
   25/10000: episode: 2, duration: 0.019s, episode steps:  11, steps per second: 590, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.508504, mae: 0.528415, mean_q: 0.054343
   94/10000: episode: 3, duration: 0.100s, episode steps:  69, steps per second: 693, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.325002, mae: 0.528633, mean_q: 0.297807
  107/10000: episode: 4, duration: 0.020s, episode steps:  13, steps per second: 659, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.084078, mae: 0.585128, mean_q: 0.822693
  126/10000: episode: 5, duration: 0.028s, episode steps:  19, steps per second: 669, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.040076, mae: 0.652247, mean_q: 1.113272
  137/10000: episode: 6, duration: 0.017s, episode steps:  11, steps per second: 645, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.031297, mae: 0.716009, mean_q: 1.274004
  149/10000: episode: 7, duration: 0.019s, episode steps:  12, steps per second: 647, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.039856, mae: 0.753450, mean_q: 1.354727
  166/10000: episode: 8, duration: 0.025s, episode steps:  17, steps per second: 667, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.044582, mae: 0.797810, mean_q: 1.442333
  176/10000: episode: 9, duration: 0.015s, episode steps:  10, steps per second: 646, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.044745, mae: 0.839846, mean_q: 1.526271
  237/10000: episode: 10, duration: 0.088s, episode steps:  61, steps per second: 689, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.042304, mae: 0.953512, mean_q: 1.786658
  256/10000: episode: 11, duration: 0.030s, episode steps:  19, steps per second: 640, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.046358, mae: 1.083539, mean_q: 2.061573
  283/10000: episode: 12, duration: 0.040s, episode steps:  27, steps per second: 680, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.050854, mae: 1.177360, mean_q: 2.257789
  301/10000: episode: 13, duration: 0.027s, episode steps:  18, steps per second: 671, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.076886, mae: 1.281120, mean_q: 2.453673
  326/10000: episode: 14, duration: 0.037s, episode steps:  25, steps per second: 668, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.060192, mae: 1.340794, mean_q: 2.595536
  359/10000: episode: 15, duration: 0.048s, episode steps:  33, steps per second: 688, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.394 [0.000, 1.000],  loss: 0.075160, mae: 1.459551, mean_q: 2.821319
  382/10000: episode: 16, duration: 0.034s, episode steps:  23, steps per second: 685, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.075416, mae: 1.558543, mean_q: 3.009653
  407/10000: episode: 17, duration: 0.036s, episode steps:  25, steps per second: 702, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.090370, mae: 1.670767, mean_q: 3.210455
  425/10000: episode: 18, duration: 0.026s, episode steps:  18, steps per second: 686, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.084703, mae: 1.749238, mean_q: 3.391618
  440/10000: episode: 19, duration: 0.022s, episode steps:  15, steps per second: 668, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.135517, mae: 1.829296, mean_q: 3.489378
  483/10000: episode: 20, duration: 0.061s, episode steps:  43, steps per second: 702, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 0.117893, mae: 1.934651, mean_q: 3.732517
  496/10000: episode: 21, duration: 0.019s, episode steps:  13, steps per second: 669, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.199028, mae: 2.070530, mean_q: 3.965053
  506/10000: episode: 22, duration: 0.015s, episode steps:  10, steps per second: 664, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.078807, mae: 2.070198, mean_q: 4.080727
  515/10000: episode: 23, duration: 0.014s, episode steps:   9, steps per second: 649, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.139370, mae: 2.164118, mean_q: 4.179774
  536/10000: episode: 24, duration: 0.031s, episode steps:  21, steps per second: 676, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.148759, mae: 2.195210, mean_q: 4.235061
  565/10000: episode: 25, duration: 0.042s, episode steps:  29, steps per second: 693, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.174341, mae: 2.300159, mean_q: 4.410788
  613/10000: episode: 26, duration: 0.068s, episode steps:  48, steps per second: 702, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.165093, mae: 2.463217, mean_q: 4.754895
  635/10000: episode: 27, duration: 0.032s, episode steps:  22, steps per second: 695, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.249844, mae: 2.594809, mean_q: 4.874410
  647/10000: episode: 28, duration: 0.018s, episode steps:  12, steps per second: 667, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.150234, mae: 2.714875, mean_q: 5.228327
  666/10000: episode: 29, duration: 0.027s, episode steps:  19, steps per second: 693, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.320916, mae: 2.735588, mean_q: 5.172807
  690/10000: episode: 30, duration: 0.034s, episode steps:  24, steps per second: 697, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.252862, mae: 2.804532, mean_q: 5.359014
  719/10000: episode: 31, duration: 0.041s, episode steps:  29, steps per second: 700, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  loss: 0.349443, mae: 2.917417, mean_q: 5.519489
  775/10000: episode: 32, duration: 0.079s, episode steps:  56, steps per second: 707, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 0.284737, mae: 3.079505, mean_q: 5.921013
  813/10000: episode: 33, duration: 0.054s, episode steps:  38, steps per second: 705, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 0.361925, mae: 3.265424, mean_q: 6.255056
  837/10000: episode: 34, duration: 0.035s, episode steps:  24, steps per second: 691, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.316169, mae: 3.358203, mean_q: 6.434692
  847/10000: episode: 35, duration: 0.015s, episode steps:  10, steps per second: 659, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.343395, mae: 3.411338, mean_q: 6.506067
  864/10000: episode: 36, duration: 0.025s, episode steps:  17, steps per second: 689, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.410670, mae: 3.496571, mean_q: 6.686577
  882/10000: episode: 37, duration: 0.026s, episode steps:  18, steps per second: 692, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.328533, mae: 3.515569, mean_q: 6.765529
  905/10000: episode: 38, duration: 0.033s, episode steps:  23, steps per second: 694, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.380896, mae: 3.611907, mean_q: 6.951642
  991/10000: episode: 39, duration: 0.121s, episode steps:  86, steps per second: 710, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.409190, mae: 3.820705, mean_q: 7.366817
 1024/10000: episode: 40, duration: 0.047s, episode steps:  33, steps per second: 708, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.383418, mae: 4.018578, mean_q: 7.779821
 1048/10000: episode: 41, duration: 0.034s, episode steps:  24, steps per second: 702, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.297459, mae: 4.129435, mean_q: 8.082279
 1131/10000: episode: 42, duration: 0.114s, episode steps:  83, steps per second: 726, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.388044, mae: 4.327434, mean_q: 8.444755
 1173/10000: episode: 43, duration: 0.059s, episode steps:  42, steps per second: 711, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.552269, mae: 4.617291, mean_q: 8.996874
 1213/10000: episode: 44, duration: 0.057s, episode steps:  40, steps per second: 702, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.544180, mae: 4.795664, mean_q: 9.396461
 1245/10000: episode: 45, duration: 0.045s, episode steps:  32, steps per second: 708, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.570207, mae: 4.926667, mean_q: 9.614830
 1270/10000: episode: 46, duration: 0.035s, episode steps:  25, steps per second: 710, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.514044, mae: 4.993756, mean_q: 9.793330
 1372/10000: episode: 47, duration: 0.140s, episode steps: 102, steps per second: 730, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.431 [0.000, 1.000],  loss: 0.535154, mae: 5.279710, mean_q: 10.395741
 1430/10000: episode: 48, duration: 0.081s, episode steps:  58, steps per second: 718, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.709934, mae: 5.607312, mean_q: 11.089118
 1454/10000: episode: 49, duration: 0.035s, episode steps:  24, steps per second: 695, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.569543, mae: 5.747958, mean_q: 11.362613
 1584/10000: episode: 50, duration: 0.179s, episode steps: 130, steps per second: 728, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.696342, mae: 6.102141, mean_q: 12.103384
 1693/10000: episode: 51, duration: 0.149s, episode steps: 109, steps per second: 731, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.736741, mae: 6.513464, mean_q: 13.001818
 1790/10000: episode: 52, duration: 0.134s, episode steps:  97, steps per second: 722, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.772942, mae: 6.917435, mean_q: 13.849023
 1822/10000: episode: 53, duration: 0.046s, episode steps:  32, steps per second: 702, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.909185, mae: 7.178185, mean_q: 14.394342
 1885/10000: episode: 54, duration: 0.087s, episode steps:  63, steps per second: 726, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 1.085115, mae: 7.377817, mean_q: 14.713846
 1931/10000: episode: 55, duration: 0.067s, episode steps:  46, steps per second: 687, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 1.039786, mae: 7.586796, mean_q: 15.171762
 1963/10000: episode: 56, duration: 0.045s, episode steps:  32, steps per second: 714, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.217593, mae: 7.810156, mean_q: 15.667741
 2060/10000: episode: 57, duration: 0.133s, episode steps:  97, steps per second: 730, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.376569, mae: 7.994944, mean_q: 15.977460
 2196/10000: episode: 58, duration: 0.187s, episode steps: 136, steps per second: 727, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.197047, mae: 8.474279, mean_q: 17.025682
 2380/10000: episode: 59, duration: 0.251s, episode steps: 184, steps per second: 733, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.285030, mae: 9.141058, mean_q: 18.468294
 2588/10000: episode: 60, duration: 0.284s, episode steps: 208, steps per second: 734, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.314636, mae: 10.016926, mean_q: 20.267361
 2819/10000: episode: 61, duration: 0.315s, episode steps: 231, steps per second: 733, episode reward: 231.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.491906, mae: 10.977439, mean_q: 22.257092
 3332/10000: episode: 62, duration: 0.696s, episode steps: 513, steps per second: 737, episode reward: 513.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.870787, mae: 12.649074, mean_q: 25.624128
 3517/10000: episode: 63, duration: 0.252s, episode steps: 185, steps per second: 733, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.921060, mae: 14.129253, mean_q: 28.650322
 3707/10000: episode: 64, duration: 0.259s, episode steps: 190, steps per second: 734, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.101708, mae: 14.877976, mean_q: 30.226858
 4042/10000: episode: 65, duration: 0.456s, episode steps: 335, steps per second: 735, episode reward: 335.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.150411, mae: 15.982522, mean_q: 32.477695
 4306/10000: episode: 66, duration: 0.364s, episode steps: 264, steps per second: 726, episode reward: 264.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.211206, mae: 17.284771, mean_q: 35.174042
 4507/10000: episode: 67, duration: 0.282s, episode steps: 201, steps per second: 713, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.765179, mae: 18.225258, mean_q: 36.922306
 4873/10000: episode: 68, duration: 0.507s, episode steps: 366, steps per second: 722, episode reward: 366.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.392021, mae: 19.349371, mean_q: 39.260555
 5120/10000: episode: 69, duration: 0.342s, episode steps: 247, steps per second: 721, episode reward: 247.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.631831, mae: 20.547951, mean_q: 41.683327
 5268/10000: episode: 70, duration: 0.247s, episode steps: 148, steps per second: 600, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 3.009722, mae: 21.213301, mean_q: 43.008801
 5467/10000: episode: 71, duration: 0.282s, episode steps: 199, steps per second: 706, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.272381, mae: 21.768536, mean_q: 44.116283
 5670/10000: episode: 72, duration: 0.290s, episode steps: 203, steps per second: 700, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.775202, mae: 22.468548, mean_q: 45.616352
 5846/10000: episode: 73, duration: 0.246s, episode steps: 176, steps per second: 715, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.218296, mae: 23.251539, mean_q: 47.170334
 6020/10000: episode: 74, duration: 0.243s, episode steps: 174, steps per second: 715, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.091964, mae: 23.696634, mean_q: 48.017948
 6292/10000: episode: 75, duration: 0.404s, episode steps: 272, steps per second: 674, episode reward: 272.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.626102, mae: 24.395473, mean_q: 49.479813
 6457/10000: episode: 76, duration: 0.233s, episode steps: 165, steps per second: 707, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 3.711124, mae: 25.106123, mean_q: 50.868225
 6694/10000: episode: 77, duration: 0.328s, episode steps: 237, steps per second: 722, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.522245, mae: 25.640749, mean_q: 51.953224
 6926/10000: episode: 78, duration: 0.350s, episode steps: 232, steps per second: 663, episode reward: 232.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.136884, mae: 26.369047, mean_q: 53.444935
 7081/10000: episode: 79, duration: 0.218s, episode steps: 155, steps per second: 710, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 3.826838, mae: 26.822451, mean_q: 54.401165
 7262/10000: episode: 80, duration: 0.251s, episode steps: 181, steps per second: 721, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 3.202940, mae: 27.326725, mean_q: 55.431759
 7425/10000: episode: 81, duration: 0.225s, episode steps: 163, steps per second: 726, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 4.021163, mae: 27.674055, mean_q: 56.065380
 7597/10000: episode: 82, duration: 0.242s, episode steps: 172, steps per second: 710, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.421058, mae: 28.177263, mean_q: 57.090633
 7798/10000: episode: 83, duration: 0.279s, episode steps: 201, steps per second: 722, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.164929, mae: 28.603590, mean_q: 58.139023
 8119/10000: episode: 84, duration: 0.444s, episode steps: 321, steps per second: 724, episode reward: 321.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.624386, mae: 29.284725, mean_q: 59.400860
 8599/10000: episode: 85, duration: 0.661s, episode steps: 480, steps per second: 727, episode reward: 480.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.684086, mae: 30.305521, mean_q: 61.508724
 8913/10000: episode: 86, duration: 0.432s, episode steps: 314, steps per second: 727, episode reward: 314.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.698453, mae: 31.620148, mean_q: 64.155792
 9074/10000: episode: 87, duration: 0.222s, episode steps: 161, steps per second: 726, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 6.078128, mae: 32.077755, mean_q: 64.951118
 9259/10000: episode: 88, duration: 0.256s, episode steps: 185, steps per second: 722, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 4.320056, mae: 32.428944, mean_q: 65.801796
 9513/10000: episode: 89, duration: 0.359s, episode steps: 254, steps per second: 708, episode reward: 254.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.325381, mae: 32.710136, mean_q: 66.458336
 9746/10000: episode: 90, duration: 0.329s, episode steps: 233, steps per second: 708, episode reward: 233.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.619481, mae: 33.134304, mean_q: 67.242538
 9951/10000: episode: 91, duration: 0.294s, episode steps: 205, steps per second: 698, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.446834, mae: 33.866215, mean_q: 68.645844
done, took 14.209 seconds
Testing for 10 episodes ...
Episode 1: reward: 204.000, steps: 204
Episode 2: reward: 340.000, steps: 340
Episode 3: reward: 217.000, steps: 217
Episode 4: reward: 271.000, steps: 271
Episode 5: reward: 296.000, steps: 296
Episode 6: reward: 289.000, steps: 289
Episode 7: reward: 198.000, steps: 198
Episode 8: reward: 284.000, steps: 284
Episode 9: reward: 199.000, steps: 199
Episode 10: reward: 226.000, steps: 226
[204.0, 340.0, 217.0, 271.0, 296.0, 289.0, 198.0, 284.0, 199.0, 226.0]
