#Run 1 - Random actions

Episode 1, score=15.0
Episode 2, score=24.0
Episode 3, score=24.0
Episode 4, score=18.0
Episode 5, score=13.0
Episode 6, score=14.0
Episode 7, score=20.0
Episode 8, score=13.0
Episode 9, score=33.0
Episode 10, score=35.0

-----------------------------------------------------------------------------------
###################################################################################
-----------------------------------------------------------------------------------
Sucessful Run 2 - Initial Training, model performs decently at task
Training for 10000 steps ...
   10/10000: episode: 1, duration: 0.042s, episode steps:  10, steps per second: 236, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: --, mae: --, mean_q: --
   25/10000: episode: 2, duration: 0.174s, episode steps:  15, steps per second:  86, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.541401, mae: 0.575754, mean_q: 0.124254
   36/10000: episode: 3, duration: 0.018s, episode steps:  11, steps per second: 610, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.510082, mae: 0.557423, mean_q: 0.192902
   46/10000: episode: 4, duration: 0.031s, episode steps:  10, steps per second: 326, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.426661, mae: 0.529925, mean_q: 0.270106
   59/10000: episode: 5, duration: 0.030s, episode steps:  13, steps per second: 431, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.387958, mae: 0.552585, mean_q: 0.393537
   75/10000: episode: 6, duration: 0.024s, episode steps:  16, steps per second: 660, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.188 [0.000, 1.000],  loss: 0.315785, mae: 0.564806, mean_q: 0.532089
   86/10000: episode: 7, duration: 0.017s, episode steps:  11, steps per second: 633, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.237316, mae: 0.570200, mean_q: 0.708419
  128/10000: episode: 8, duration: 0.062s, episode steps:  42, steps per second: 678, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.174496, mae: 0.670777, mean_q: 0.987250
  195/10000: episode: 9, duration: 0.098s, episode steps:  67, steps per second: 685, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.110385, mae: 0.842089, mean_q: 1.394182
  206/10000: episode: 10, duration: 0.017s, episode steps:  11, steps per second: 634, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.084145, mae: 0.960348, mean_q: 1.765874
  227/10000: episode: 11, duration: 0.031s, episode steps:  21, steps per second: 669, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.097169, mae: 1.023536, mean_q: 1.913382
  270/10000: episode: 12, duration: 0.062s, episode steps:  43, steps per second: 696, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.651 [0.000, 1.000],  loss: 0.065232, mae: 1.101017, mean_q: 2.148717
  293/10000: episode: 13, duration: 0.033s, episode steps:  23, steps per second: 690, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.066754, mae: 1.215266, mean_q: 2.377074
  315/10000: episode: 14, duration: 0.032s, episode steps:  22, steps per second: 695, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.124222, mae: 1.364251, mean_q: 2.678797
  341/10000: episode: 15, duration: 0.037s, episode steps:  26, steps per second: 703, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 0.134807, mae: 1.449741, mean_q: 2.795140
  357/10000: episode: 16, duration: 0.024s, episode steps:  16, steps per second: 679, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.061533, mae: 1.499655, mean_q: 2.980739
  386/10000: episode: 17, duration: 0.041s, episode steps:  29, steps per second: 700, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.135575, mae: 1.629689, mean_q: 3.206405
  414/10000: episode: 18, duration: 0.040s, episode steps:  28, steps per second: 706, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.100968, mae: 1.743940, mean_q: 3.429747
  432/10000: episode: 19, duration: 0.029s, episode steps:  18, steps per second: 618, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.178861, mae: 1.852827, mean_q: 3.602498
  442/10000: episode: 20, duration: 0.016s, episode steps:  10, steps per second: 633, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.174393, mae: 1.908911, mean_q: 3.701096
  513/10000: episode: 21, duration: 0.102s, episode steps:  71, steps per second: 696, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.606 [0.000, 1.000],  loss: 0.205148, mae: 2.081444, mean_q: 4.018554
  524/10000: episode: 22, duration: 0.017s, episode steps:  11, steps per second: 644, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.253732, mae: 2.261609, mean_q: 4.377188
  536/10000: episode: 23, duration: 0.018s, episode steps:  12, steps per second: 657, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.194455, mae: 2.299708, mean_q: 4.482961
  554/10000: episode: 24, duration: 0.027s, episode steps:  18, steps per second: 675, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.241685, mae: 2.356759, mean_q: 4.604324
  586/10000: episode: 25, duration: 0.047s, episode steps:  32, steps per second: 686, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.279666, mae: 2.483194, mean_q: 4.770270
  659/10000: episode: 26, duration: 0.104s, episode steps:  73, steps per second: 705, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.273558, mae: 2.698374, mean_q: 5.230793
  670/10000: episode: 27, duration: 0.017s, episode steps:  11, steps per second: 644, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.324457, mae: 2.891985, mean_q: 5.629799
  679/10000: episode: 28, duration: 0.014s, episode steps:   9, steps per second: 634, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.408542, mae: 2.940808, mean_q: 5.601155
  702/10000: episode: 29, duration: 0.033s, episode steps:  23, steps per second: 690, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.382699, mae: 2.985716, mean_q: 5.771744
  713/10000: episode: 30, duration: 0.017s, episode steps:  11, steps per second: 665, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.429561, mae: 3.113783, mean_q: 5.928917
  741/10000: episode: 31, duration: 0.040s, episode steps:  28, steps per second: 704, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.415924, mae: 3.139836, mean_q: 6.033495
  768/10000: episode: 32, duration: 0.038s, episode steps:  27, steps per second: 702, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.436081, mae: 3.264763, mean_q: 6.221168
  785/10000: episode: 33, duration: 0.025s, episode steps:  17, steps per second: 685, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.471145, mae: 3.333996, mean_q: 6.334532
  803/10000: episode: 34, duration: 0.026s, episode steps:  18, steps per second: 682, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.426806, mae: 3.397216, mean_q: 6.533540
  865/10000: episode: 35, duration: 0.088s, episode steps:  62, steps per second: 707, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.429466, mae: 3.546180, mean_q: 6.783129
  882/10000: episode: 36, duration: 0.025s, episode steps:  17, steps per second: 674, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.558947, mae: 3.724086, mean_q: 7.082577
  899/10000: episode: 37, duration: 0.025s, episode steps:  17, steps per second: 683, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.524196, mae: 3.766965, mean_q: 7.232659
  914/10000: episode: 38, duration: 0.022s, episode steps:  15, steps per second: 683, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.595411, mae: 3.807205, mean_q: 7.212662
  943/10000: episode: 39, duration: 0.041s, episode steps:  29, steps per second: 709, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.488069, mae: 3.887258, mean_q: 7.430610
  974/10000: episode: 40, duration: 0.044s, episode steps:  31, steps per second: 711, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 0.641952, mae: 4.031791, mean_q: 7.736495
  990/10000: episode: 41, duration: 0.023s, episode steps:  16, steps per second: 682, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.466292, mae: 4.076171, mean_q: 7.926237
 1009/10000: episode: 42, duration: 0.027s, episode steps:  19, steps per second: 699, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.577858, mae: 4.169096, mean_q: 8.058824
 1054/10000: episode: 43, duration: 0.062s, episode steps:  45, steps per second: 723, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.675623, mae: 4.279622, mean_q: 8.170765
 1071/10000: episode: 44, duration: 0.025s, episode steps:  17, steps per second: 693, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.665826, mae: 4.364753, mean_q: 8.340804
 1085/10000: episode: 45, duration: 0.020s, episode steps:  14, steps per second: 683, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.952939, mae: 4.423165, mean_q: 8.334531
 1144/10000: episode: 46, duration: 0.081s, episode steps:  59, steps per second: 728, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.638217, mae: 4.571093, mean_q: 8.823207
 1162/10000: episode: 47, duration: 0.026s, episode steps:  18, steps per second: 686, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.609267, mae: 4.713482, mean_q: 9.141719
 1176/10000: episode: 48, duration: 0.020s, episode steps:  14, steps per second: 686, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.829275, mae: 4.747272, mean_q: 9.069180
 1201/10000: episode: 49, duration: 0.035s, episode steps:  25, steps per second: 708, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.709477, mae: 4.876617, mean_q: 9.417481
 1261/10000: episode: 50, duration: 0.083s, episode steps:  60, steps per second: 723, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.627803, mae: 5.026846, mean_q: 9.800150
 1318/10000: episode: 51, duration: 0.078s, episode steps:  57, steps per second: 727, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.753056, mae: 5.238380, mean_q: 10.232036
 1362/10000: episode: 52, duration: 0.061s, episode steps:  44, steps per second: 719, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.386 [0.000, 1.000],  loss: 0.802153, mae: 5.441518, mean_q: 10.596675
 1380/10000: episode: 53, duration: 0.026s, episode steps:  18, steps per second: 694, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.299074, mae: 5.501449, mean_q: 10.501891
 1441/10000: episode: 54, duration: 0.084s, episode steps:  61, steps per second: 727, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.855318, mae: 5.714109, mean_q: 11.124003
 1473/10000: episode: 55, duration: 0.045s, episode steps:  32, steps per second: 713, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.770002, mae: 5.820418, mean_q: 11.424055
 1539/10000: episode: 56, duration: 0.091s, episode steps:  66, steps per second: 729, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.847033, mae: 6.031555, mean_q: 11.845492
 1551/10000: episode: 57, duration: 0.018s, episode steps:  12, steps per second: 684, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.993792, mae: 6.243186, mean_q: 12.263389
 1592/10000: episode: 58, duration: 0.057s, episode steps:  41, steps per second: 717, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 1.195403, mae: 6.271921, mean_q: 12.228886
 1611/10000: episode: 59, duration: 0.027s, episode steps:  19, steps per second: 698, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.991301, mae: 6.333229, mean_q: 12.416513
 1627/10000: episode: 60, duration: 0.023s, episode steps:  16, steps per second: 701, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.838157, mae: 6.386878, mean_q: 12.505041
 1737/10000: episode: 61, duration: 0.153s, episode steps: 110, steps per second: 721, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 1.032504, mae: 6.744028, mean_q: 13.269953
 1816/10000: episode: 62, duration: 0.112s, episode steps:  79, steps per second: 704, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.305223, mae: 7.027132, mean_q: 13.801242
 1900/10000: episode: 63, duration: 0.116s, episode steps:  84, steps per second: 722, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.464309, mae: 7.342833, mean_q: 14.447468
 1937/10000: episode: 64, duration: 0.053s, episode steps:  37, steps per second: 693, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.914628, mae: 7.621375, mean_q: 15.054819
 2140/10000: episode: 65, duration: 0.278s, episode steps: 203, steps per second: 730, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.189690, mae: 8.074594, mean_q: 16.032156
 2261/10000: episode: 66, duration: 0.166s, episode steps: 121, steps per second: 730, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.276762, mae: 8.673528, mean_q: 17.271770
 2285/10000: episode: 67, duration: 0.034s, episode steps:  24, steps per second: 708, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.489749, mae: 8.903174, mean_q: 17.697294
 2454/10000: episode: 68, duration: 0.230s, episode steps: 169, steps per second: 735, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.360788, mae: 9.261448, mean_q: 18.424606
 2613/10000: episode: 69, duration: 0.216s, episode steps: 159, steps per second: 736, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.756471, mae: 9.893058, mean_q: 19.695927
 2773/10000: episode: 70, duration: 0.219s, episode steps: 160, steps per second: 730, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.310347, mae: 10.495322, mean_q: 21.044559
 2899/10000: episode: 71, duration: 0.174s, episode steps: 126, steps per second: 725, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.803750, mae: 11.013766, mean_q: 22.042030
 3050/10000: episode: 72, duration: 0.206s, episode steps: 151, steps per second: 734, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.689919, mae: 11.496187, mean_q: 23.002733
 3247/10000: episode: 73, duration: 0.267s, episode steps: 197, steps per second: 738, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 1.880868, mae: 12.190025, mean_q: 24.499521
 3462/10000: episode: 74, duration: 0.296s, episode steps: 215, steps per second: 727, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.010069, mae: 13.014342, mean_q: 26.247375
 3628/10000: episode: 75, duration: 0.229s, episode steps: 166, steps per second: 726, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.401343, mae: 13.692188, mean_q: 27.555582
 3865/10000: episode: 76, duration: 0.327s, episode steps: 237, steps per second: 726, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.284389, mae: 14.404552, mean_q: 29.138885
 4032/10000: episode: 77, duration: 0.271s, episode steps: 167, steps per second: 616, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.991604, mae: 15.239760, mean_q: 30.941692
 4212/10000: episode: 78, duration: 0.253s, episode steps: 180, steps per second: 712, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.050869, mae: 15.901704, mean_q: 32.156868
 4382/10000: episode: 79, duration: 0.239s, episode steps: 170, steps per second: 710, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.790934, mae: 16.588026, mean_q: 33.715809
 4576/10000: episode: 80, duration: 0.266s, episode steps: 194, steps per second: 728, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.732420, mae: 17.282572, mean_q: 35.068111
 4796/10000: episode: 81, duration: 0.306s, episode steps: 220, steps per second: 719, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.234058, mae: 18.135603, mean_q: 36.925556
 5006/10000: episode: 82, duration: 0.293s, episode steps: 210, steps per second: 718, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.060503, mae: 19.045732, mean_q: 38.846615
 5206/10000: episode: 83, duration: 0.280s, episode steps: 200, steps per second: 715, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 3.167404, mae: 19.898619, mean_q: 40.441250
 5407/10000: episode: 84, duration: 0.281s, episode steps: 201, steps per second: 716, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.395657, mae: 20.717484, mean_q: 42.122181
 5582/10000: episode: 85, duration: 0.240s, episode steps: 175, steps per second: 729, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.656960, mae: 21.472599, mean_q: 43.558392
 5785/10000: episode: 86, duration: 0.278s, episode steps: 203, steps per second: 731, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.911602, mae: 22.133438, mean_q: 44.930813
 6271/10000: episode: 87, duration: 0.668s, episode steps: 486, steps per second: 727, episode reward: 486.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.542707, mae: 23.359447, mean_q: 47.442074
 6480/10000: episode: 88, duration: 0.290s, episode steps: 209, steps per second: 720, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.041703, mae: 24.512800, mean_q: 49.782501
 6692/10000: episode: 89, duration: 0.292s, episode steps: 212, steps per second: 726, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.648940, mae: 25.295357, mean_q: 51.415501
 6893/10000: episode: 90, duration: 0.285s, episode steps: 201, steps per second: 706, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.383620, mae: 26.107136, mean_q: 53.032383
 7085/10000: episode: 91, duration: 0.271s, episode steps: 192, steps per second: 709, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.645577, mae: 26.514837, mean_q: 53.840290
 7297/10000: episode: 92, duration: 0.301s, episode steps: 212, steps per second: 705, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.974399, mae: 27.142048, mean_q: 55.021820
 7501/10000: episode: 93, duration: 0.288s, episode steps: 204, steps per second: 709, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.906524, mae: 27.878231, mean_q: 56.565804
 7676/10000: episode: 94, duration: 0.247s, episode steps: 175, steps per second: 710, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.104782, mae: 28.237299, mean_q: 57.352768
 7933/10000: episode: 95, duration: 0.364s, episode steps: 257, steps per second: 706, episode reward: 257.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.380011, mae: 28.945087, mean_q: 58.836628
 8122/10000: episode: 96, duration: 0.271s, episode steps: 189, steps per second: 698, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.015730, mae: 29.648540, mean_q: 60.212353
 8300/10000: episode: 97, duration: 0.255s, episode steps: 178, steps per second: 698, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 4.790098, mae: 30.139498, mean_q: 61.161045
 8637/10000: episode: 98, duration: 0.473s, episode steps: 337, steps per second: 712, episode reward: 337.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 4.678094, mae: 30.862923, mean_q: 62.531979
 8809/10000: episode: 99, duration: 0.241s, episode steps: 172, steps per second: 714, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.222992, mae: 31.620075, mean_q: 64.145721
 9050/10000: episode: 100, duration: 0.364s, episode steps: 241, steps per second: 662, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.486578, mae: 32.249725, mean_q: 65.474495
 9250/10000: episode: 101, duration: 0.277s, episode steps: 200, steps per second: 722, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 5.387255, mae: 32.620548, mean_q: 66.005859
 9465/10000: episode: 102, duration: 0.294s, episode steps: 215, steps per second: 730, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.038850, mae: 33.291191, mean_q: 67.417992
 9668/10000: episode: 103, duration: 0.278s, episode steps: 203, steps per second: 731, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.495684, mae: 33.512306, mean_q: 67.896889
 9870/10000: episode: 104, duration: 0.277s, episode steps: 202, steps per second: 729, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 4.792703, mae: 34.142387, mean_q: 69.151794
done, took 14.252 seconds
Testing for 10 episodes ...
Episode 1: reward: 307.000, steps: 307
Episode 2: reward: 164.000, steps: 164
Episode 3: reward: 184.000, steps: 184
Episode 4: reward: 186.000, steps: 186
Episode 5: reward: 173.000, steps: 173
Episode 6: reward: 164.000, steps: 164
Episode 7: reward: 209.000, steps: 209
Episode 8: reward: 173.000, steps: 173
Episode 9: reward: 239.000, steps: 239
Episode 10: reward: 197.000, steps: 197
[307.0, 164.0, 184.0, 186.0, 173.0, 164.0, 209.0, 173.0, 239.0, 197.0]




-----------------------------------------------------------------------------------
###################################################################################
-----------------------------------------------------------------------------------
Sucessful Run 3 - Longer training, model near perfect at Task

Training for 100000 steps ...
    12/100000: episode: 1, duration: 0.178s, episode steps:  12, steps per second:  68, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.573633, mae: 0.564542, mean_q: -0.002276
    42/100000: episode: 2, duration: 0.046s, episode steps:  30, steps per second: 650, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.395459, mae: 0.518120, mean_q: 0.179069
    82/100000: episode: 3, duration: 0.057s, episode steps:  40, steps per second: 702, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.198024, mae: 0.549934, mean_q: 0.599730
    98/100000: episode: 4, duration: 0.024s, episode steps:  16, steps per second: 676, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.149506, mae: 0.649850, mean_q: 0.983663
   135/100000: episode: 5, duration: 0.053s, episode steps:  37, steps per second: 702, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.119895, mae: 0.717937, mean_q: 1.180678
   160/100000: episode: 6, duration: 0.036s, episode steps:  25, steps per second: 692, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.089516, mae: 0.812117, mean_q: 1.491401
   170/100000: episode: 7, duration: 0.015s, episode steps:  10, steps per second: 657, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.059903, mae: 0.871201, mean_q: 1.680252
   188/100000: episode: 8, duration: 0.026s, episode steps:  18, steps per second: 685, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.056247, mae: 0.947016, mean_q: 1.846928
   203/100000: episode: 9, duration: 0.022s, episode steps:  15, steps per second: 673, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.068883, mae: 1.010193, mean_q: 1.957115
   232/100000: episode: 10, duration: 0.042s, episode steps:  29, steps per second: 698, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.050224, mae: 1.068316, mean_q: 2.149299
   248/100000: episode: 11, duration: 0.023s, episode steps:  16, steps per second: 688, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.074174, mae: 1.181582, mean_q: 2.340031
   274/100000: episode: 12, duration: 0.037s, episode steps:  26, steps per second: 696, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.071958, mae: 1.269383, mean_q: 2.521893
   293/100000: episode: 13, duration: 0.028s, episode steps:  19, steps per second: 687, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.083573, mae: 1.369786, mean_q: 2.719889
   317/100000: episode: 14, duration: 0.035s, episode steps:  24, steps per second: 690, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.101275, mae: 1.467827, mean_q: 2.866245
   338/100000: episode: 15, duration: 0.030s, episode steps:  21, steps per second: 693, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.086121, mae: 1.531162, mean_q: 3.037954
   351/100000: episode: 16, duration: 0.019s, episode steps:  13, steps per second: 672, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.096974, mae: 1.605955, mean_q: 3.112960
   362/100000: episode: 17, duration: 0.016s, episode steps:  11, steps per second: 680, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.078736, mae: 1.655429, mean_q: 3.245628
   377/100000: episode: 18, duration: 0.022s, episode steps:  15, steps per second: 680, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.132572, mae: 1.739157, mean_q: 3.374687
   389/100000: episode: 19, duration: 0.018s, episode steps:  12, steps per second: 665, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.101221, mae: 1.795827, mean_q: 3.513168
   408/100000: episode: 20, duration: 0.028s, episode steps:  19, steps per second: 688, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.174684, mae: 1.870972, mean_q: 3.558676
   420/100000: episode: 21, duration: 0.018s, episode steps:  12, steps per second: 669, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.173379, mae: 1.933093, mean_q: 3.753695
   444/100000: episode: 22, duration: 0.034s, episode steps:  24, steps per second: 701, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.203982, mae: 2.029577, mean_q: 3.810093
   460/100000: episode: 23, duration: 0.023s, episode steps:  16, steps per second: 685, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.196490, mae: 2.078692, mean_q: 3.887781
   482/100000: episode: 24, duration: 0.032s, episode steps:  22, steps per second: 689, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.178025, mae: 2.154358, mean_q: 4.107096
   504/100000: episode: 25, duration: 0.032s, episode steps:  22, steps per second: 697, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.183022, mae: 2.225631, mean_q: 4.242086
   549/100000: episode: 26, duration: 0.063s, episode steps:  45, steps per second: 716, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.192341, mae: 2.373699, mean_q: 4.576601
   572/100000: episode: 27, duration: 0.033s, episode steps:  23, steps per second: 707, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.249329, mae: 2.508335, mean_q: 4.791542
   588/100000: episode: 28, duration: 0.024s, episode steps:  16, steps per second: 678, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.248117, mae: 2.601334, mean_q: 4.989512
   600/100000: episode: 29, duration: 0.018s, episode steps:  12, steps per second: 677, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.280440, mae: 2.677210, mean_q: 5.117795
   623/100000: episode: 30, duration: 0.033s, episode steps:  23, steps per second: 704, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.284470, mae: 2.703926, mean_q: 5.143872
   668/100000: episode: 31, duration: 0.063s, episode steps:  45, steps per second: 718, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.284760, mae: 2.860905, mean_q: 5.478230
   684/100000: episode: 32, duration: 0.023s, episode steps:  16, steps per second: 683, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.247649, mae: 2.951183, mean_q: 5.699646
   712/100000: episode: 33, duration: 0.040s, episode steps:  28, steps per second: 706, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.345493, mae: 3.058719, mean_q: 5.851359
   730/100000: episode: 34, duration: 0.026s, episode steps:  18, steps per second: 693, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.429918, mae: 3.152917, mean_q: 5.961728
   767/100000: episode: 35, duration: 0.052s, episode steps:  37, steps per second: 712, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  loss: 0.282750, mae: 3.255847, mean_q: 6.266301
   781/100000: episode: 36, duration: 0.020s, episode steps:  14, steps per second: 689, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.359560, mae: 3.372006, mean_q: 6.523296
   826/100000: episode: 37, duration: 0.063s, episode steps:  45, steps per second: 717, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.385346, mae: 3.466583, mean_q: 6.648754
   844/100000: episode: 38, duration: 0.026s, episode steps:  18, steps per second: 700, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.337189, mae: 3.570386, mean_q: 6.882877
   869/100000: episode: 39, duration: 0.036s, episode steps:  25, steps per second: 703, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.288066, mae: 3.638679, mean_q: 7.093358
   901/100000: episode: 40, duration: 0.045s, episode steps:  32, steps per second: 716, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.459315, mae: 3.748908, mean_q: 7.252203
   930/100000: episode: 41, duration: 0.041s, episode steps:  29, steps per second: 710, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.387698, mae: 3.868527, mean_q: 7.478281
   948/100000: episode: 42, duration: 0.026s, episode steps:  18, steps per second: 703, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.270264, mae: 3.974018, mean_q: 7.776323
   971/100000: episode: 43, duration: 0.032s, episode steps:  23, steps per second: 713, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.390453, mae: 4.009065, mean_q: 7.793646
  1001/100000: episode: 44, duration: 0.041s, episode steps:  30, steps per second: 738, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.467111, mae: 4.125249, mean_q: 8.023037
  1050/100000: episode: 45, duration: 0.067s, episode steps:  49, steps per second: 729, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 0.527014, mae: 4.231790, mean_q: 8.176675
  1086/100000: episode: 46, duration: 0.050s, episode steps:  36, steps per second: 718, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.333246, mae: 4.437839, mean_q: 8.715933
  1120/100000: episode: 47, duration: 0.047s, episode steps:  34, steps per second: 716, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.345226, mae: 4.527625, mean_q: 8.953458
  1133/100000: episode: 48, duration: 0.019s, episode steps:  13, steps per second: 683, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.523207, mae: 4.630028, mean_q: 9.086606
  1207/100000: episode: 49, duration: 0.102s, episode steps:  74, steps per second: 727, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 0.507114, mae: 4.776220, mean_q: 9.379812
  1221/100000: episode: 50, duration: 0.020s, episode steps:  14, steps per second: 685, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.451070, mae: 4.965835, mean_q: 9.805204
  1253/100000: episode: 51, duration: 0.045s, episode steps:  32, steps per second: 715, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.546508, mae: 5.055880, mean_q: 10.005146
  1283/100000: episode: 52, duration: 0.042s, episode steps:  30, steps per second: 715, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.598594, mae: 5.126495, mean_q: 10.129224
  1322/100000: episode: 53, duration: 0.054s, episode steps:  39, steps per second: 721, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.420753, mae: 5.359732, mean_q: 10.650826
  1334/100000: episode: 54, duration: 0.017s, episode steps:  12, steps per second: 700, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.735501, mae: 5.468878, mean_q: 10.775313
  1370/100000: episode: 55, duration: 0.050s, episode steps:  36, steps per second: 720, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.483832, mae: 5.511066, mean_q: 10.989625
  1425/100000: episode: 56, duration: 0.076s, episode steps:  55, steps per second: 726, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.688284, mae: 5.687760, mean_q: 11.260575
  1496/100000: episode: 57, duration: 0.098s, episode steps:  71, steps per second: 725, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.679727, mae: 5.899511, mean_q: 11.706853
  1541/100000: episode: 58, duration: 0.064s, episode steps:  45, steps per second: 707, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.733788, mae: 6.120228, mean_q: 12.139238
  1588/100000: episode: 59, duration: 0.067s, episode steps:  47, steps per second: 699, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 0.578511, mae: 6.332659, mean_q: 12.667083
  1668/100000: episode: 60, duration: 0.112s, episode steps:  80, steps per second: 712, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.806342, mae: 6.628357, mean_q: 13.206955
  1714/100000: episode: 61, duration: 0.064s, episode steps:  46, steps per second: 721, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.077209, mae: 6.861773, mean_q: 13.583380
  1745/100000: episode: 62, duration: 0.045s, episode steps:  31, steps per second: 692, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.753212, mae: 6.975995, mean_q: 13.969381
  1814/100000: episode: 63, duration: 0.096s, episode steps:  69, steps per second: 722, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.707054, mae: 7.126552, mean_q: 14.325720
  1893/100000: episode: 64, duration: 0.108s, episode steps:  79, steps per second: 730, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.874570, mae: 7.342419, mean_q: 14.715664
  2025/100000: episode: 65, duration: 0.182s, episode steps: 132, steps per second: 727, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.030807, mae: 7.775277, mean_q: 15.548048
  2087/100000: episode: 66, duration: 0.087s, episode steps:  62, steps per second: 710, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.934018, mae: 8.086244, mean_q: 16.287340
  2153/100000: episode: 67, duration: 0.092s, episode steps:  66, steps per second: 714, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.107498, mae: 8.298752, mean_q: 16.723192
  2214/100000: episode: 68, duration: 0.085s, episode steps:  61, steps per second: 717, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 1.066337, mae: 8.504679, mean_q: 17.156551
  2320/100000: episode: 69, duration: 0.148s, episode steps: 106, steps per second: 716, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.635385, mae: 8.904055, mean_q: 18.115417
  2388/100000: episode: 70, duration: 0.095s, episode steps:  68, steps per second: 714, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 0.893061, mae: 9.105446, mean_q: 18.527281
  2463/100000: episode: 71, duration: 0.105s, episode steps:  75, steps per second: 717, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.805043, mae: 9.549696, mean_q: 19.535975
  2594/100000: episode: 72, duration: 0.181s, episode steps: 131, steps per second: 723, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.057107, mae: 9.872457, mean_q: 20.096109
  2702/100000: episode: 73, duration: 0.148s, episode steps: 108, steps per second: 729, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.074942, mae: 10.478606, mean_q: 21.292309
  2789/100000: episode: 74, duration: 0.119s, episode steps:  87, steps per second: 730, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.886171, mae: 10.835267, mean_q: 22.136599
  2905/100000: episode: 75, duration: 0.158s, episode steps: 116, steps per second: 734, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.860687, mae: 11.163233, mean_q: 22.819424
  3029/100000: episode: 76, duration: 0.170s, episode steps: 124, steps per second: 731, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.232644, mae: 11.780217, mean_q: 24.001003
  3154/100000: episode: 77, duration: 0.176s, episode steps: 125, steps per second: 711, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 1.147974, mae: 12.351018, mean_q: 25.199446
  3246/100000: episode: 78, duration: 0.128s, episode steps:  92, steps per second: 721, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.078076, mae: 12.649099, mean_q: 25.812231
  3379/100000: episode: 79, duration: 0.182s, episode steps: 133, steps per second: 733, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.339242, mae: 13.140815, mean_q: 26.809034
  3545/100000: episode: 80, duration: 0.231s, episode steps: 166, steps per second: 720, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.292466, mae: 13.804596, mean_q: 28.154638
  3675/100000: episode: 81, duration: 0.180s, episode steps: 130, steps per second: 722, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.250155, mae: 14.349478, mean_q: 29.303200
  3796/100000: episode: 82, duration: 0.168s, episode steps: 121, steps per second: 722, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.148806, mae: 14.837070, mean_q: 30.364840
  3951/100000: episode: 83, duration: 0.215s, episode steps: 155, steps per second: 721, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.354951, mae: 15.511854, mean_q: 31.661640
  4123/100000: episode: 84, duration: 0.236s, episode steps: 172, steps per second: 728, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.646361, mae: 15.963949, mean_q: 32.537033
  4344/100000: episode: 85, duration: 0.339s, episode steps: 221, steps per second: 651, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.513765, mae: 16.806330, mean_q: 34.365349
  4520/100000: episode: 86, duration: 0.239s, episode steps: 176, steps per second: 736, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.419431, mae: 17.501501, mean_q: 35.720745
  4746/100000: episode: 87, duration: 0.308s, episode steps: 226, steps per second: 735, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.462550, mae: 18.278196, mean_q: 37.346638
  4985/100000: episode: 88, duration: 0.325s, episode steps: 239, steps per second: 736, episode reward: 239.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.390941, mae: 19.231113, mean_q: 39.323467
  5155/100000: episode: 89, duration: 0.239s, episode steps: 170, steps per second: 711, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.791874, mae: 20.074045, mean_q: 41.004673
  5413/100000: episode: 90, duration: 0.354s, episode steps: 258, steps per second: 728, episode reward: 258.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.731402, mae: 20.839209, mean_q: 42.453671
  5626/100000: episode: 91, duration: 0.296s, episode steps: 213, steps per second: 719, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.783265, mae: 21.623312, mean_q: 44.077847
  5864/100000: episode: 92, duration: 0.329s, episode steps: 238, steps per second: 724, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.170076, mae: 22.391006, mean_q: 45.603844
  5972/100000: episode: 93, duration: 0.149s, episode steps: 108, steps per second: 725, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 1.862071, mae: 22.933372, mean_q: 46.724491
  6205/100000: episode: 94, duration: 0.330s, episode steps: 233, steps per second: 706, episode reward: 233.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.799421, mae: 23.590834, mean_q: 48.091324
  6414/100000: episode: 95, duration: 0.292s, episode steps: 209, steps per second: 715, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.373927, mae: 24.157736, mean_q: 49.104786
  6590/100000: episode: 96, duration: 0.246s, episode steps: 176, steps per second: 715, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.905598, mae: 24.858782, mean_q: 50.609474
  6788/100000: episode: 97, duration: 0.271s, episode steps: 198, steps per second: 731, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.363902, mae: 25.392050, mean_q: 51.734409
  7000/100000: episode: 98, duration: 0.289s, episode steps: 212, steps per second: 734, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.816212, mae: 26.090988, mean_q: 53.019871
  7178/100000: episode: 99, duration: 0.240s, episode steps: 178, steps per second: 741, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.267855, mae: 26.623396, mean_q: 54.010593
  7356/100000: episode: 100, duration: 0.251s, episode steps: 178, steps per second: 709, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.495697, mae: 27.124630, mean_q: 55.110416
  7515/100000: episode: 101, duration: 0.230s, episode steps: 159, steps per second: 691, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.990144, mae: 27.583296, mean_q: 55.990685
  7652/100000: episode: 102, duration: 0.189s, episode steps: 137, steps per second: 726, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.374553, mae: 28.010750, mean_q: 56.804558
  7783/100000: episode: 103, duration: 0.179s, episode steps: 131, steps per second: 730, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 2.119923, mae: 28.317461, mean_q: 57.489410
  7948/100000: episode: 104, duration: 0.226s, episode steps: 165, steps per second: 729, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.969425, mae: 28.683212, mean_q: 58.164909
  8110/100000: episode: 105, duration: 0.221s, episode steps: 162, steps per second: 732, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.179470, mae: 28.830799, mean_q: 58.468594
  8377/100000: episode: 106, duration: 0.364s, episode steps: 267, steps per second: 733, episode reward: 267.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.731465, mae: 29.172224, mean_q: 59.129410
  8603/100000: episode: 107, duration: 0.308s, episode steps: 226, steps per second: 735, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.209430, mae: 29.935015, mean_q: 60.711300
  8816/100000: episode: 108, duration: 0.291s, episode steps: 213, steps per second: 732, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.993900, mae: 30.661352, mean_q: 62.177422
  8989/100000: episode: 109, duration: 0.242s, episode steps: 173, steps per second: 716, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.274764, mae: 30.400486, mean_q: 61.704369
  9141/100000: episode: 110, duration: 0.237s, episode steps: 152, steps per second: 642, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 2.755445, mae: 30.947189, mean_q: 62.799854
  9303/100000: episode: 111, duration: 0.226s, episode steps: 162, steps per second: 718, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.406099, mae: 31.207146, mean_q: 63.193882
  9471/100000: episode: 112, duration: 0.239s, episode steps: 168, steps per second: 703, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.233089, mae: 31.470907, mean_q: 63.655331
  9665/100000: episode: 113, duration: 0.269s, episode steps: 194, steps per second: 721, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.062657, mae: 31.736511, mean_q: 64.162193
  9836/100000: episode: 114, duration: 0.235s, episode steps: 171, steps per second: 726, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.275073, mae: 31.832888, mean_q: 64.385841
 10011/100000: episode: 115, duration: 0.246s, episode steps: 175, steps per second: 713, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.751140, mae: 32.328564, mean_q: 65.409676
 10197/100000: episode: 116, duration: 0.278s, episode steps: 186, steps per second: 669, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.324219, mae: 32.541821, mean_q: 65.863899
 10374/100000: episode: 117, duration: 0.255s, episode steps: 177, steps per second: 695, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.620632, mae: 32.854298, mean_q: 66.556633
 10563/100000: episode: 118, duration: 0.263s, episode steps: 189, steps per second: 718, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 2.076700, mae: 33.078255, mean_q: 67.005981
 10727/100000: episode: 119, duration: 0.233s, episode steps: 164, steps per second: 703, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.313356, mae: 33.319805, mean_q: 67.380356
 10873/100000: episode: 120, duration: 0.201s, episode steps: 146, steps per second: 725, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 1.929144, mae: 33.394577, mean_q: 67.530594
 11051/100000: episode: 121, duration: 0.246s, episode steps: 178, steps per second: 723, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.375880, mae: 33.513294, mean_q: 67.914566
 11241/100000: episode: 122, duration: 0.267s, episode steps: 190, steps per second: 712, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.059213, mae: 33.825314, mean_q: 68.487526
 11461/100000: episode: 123, duration: 0.311s, episode steps: 220, steps per second: 708, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.578730, mae: 34.021610, mean_q: 68.887794
 11709/100000: episode: 124, duration: 0.351s, episode steps: 248, steps per second: 706, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.347493, mae: 34.689674, mean_q: 70.242790
 11881/100000: episode: 125, duration: 0.261s, episode steps: 172, steps per second: 659, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.837195, mae: 35.072140, mean_q: 70.954224
 12055/100000: episode: 126, duration: 0.250s, episode steps: 174, steps per second: 695, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.812696, mae: 35.123928, mean_q: 71.142761
 12236/100000: episode: 127, duration: 0.256s, episode steps: 181, steps per second: 706, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.641508, mae: 35.125954, mean_q: 71.086494
 12448/100000: episode: 128, duration: 0.298s, episode steps: 212, steps per second: 711, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.257735, mae: 35.451828, mean_q: 71.832237
 12642/100000: episode: 129, duration: 0.278s, episode steps: 194, steps per second: 697, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.814336, mae: 35.299473, mean_q: 71.527756
 12873/100000: episode: 130, duration: 0.325s, episode steps: 231, steps per second: 710, episode reward: 231.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.126589, mae: 35.990505, mean_q: 72.873428
 13079/100000: episode: 131, duration: 0.285s, episode steps: 206, steps per second: 724, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.216172, mae: 36.297249, mean_q: 73.539658
 13257/100000: episode: 132, duration: 0.245s, episode steps: 178, steps per second: 727, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.657056, mae: 36.103863, mean_q: 73.126144
 13461/100000: episode: 133, duration: 0.281s, episode steps: 204, steps per second: 725, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 1.631585, mae: 36.507000, mean_q: 73.953918
 13627/100000: episode: 134, duration: 0.228s, episode steps: 166, steps per second: 728, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.947317, mae: 36.740314, mean_q: 74.527992
 13827/100000: episode: 135, duration: 0.275s, episode steps: 200, steps per second: 726, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.030858, mae: 37.024086, mean_q: 75.033333
 13999/100000: episode: 136, duration: 0.240s, episode steps: 172, steps per second: 717, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.436309, mae: 36.630821, mean_q: 74.121979
 14207/100000: episode: 137, duration: 0.298s, episode steps: 208, steps per second: 698, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.534407, mae: 36.724533, mean_q: 74.392036
 14396/100000: episode: 138, duration: 0.264s, episode steps: 189, steps per second: 716, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 2.216781, mae: 37.373207, mean_q: 75.630432
 14548/100000: episode: 139, duration: 0.220s, episode steps: 152, steps per second: 691, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.909329, mae: 37.055214, mean_q: 74.904129
 14745/100000: episode: 140, duration: 0.278s, episode steps: 197, steps per second: 710, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.313377, mae: 37.382015, mean_q: 75.694550
 14900/100000: episode: 141, duration: 0.222s, episode steps: 155, steps per second: 698, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.430868, mae: 37.412910, mean_q: 75.721458
 15066/100000: episode: 142, duration: 0.238s, episode steps: 166, steps per second: 698, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.910314, mae: 37.563717, mean_q: 76.034782
 15222/100000: episode: 143, duration: 0.220s, episode steps: 156, steps per second: 710, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 1.896556, mae: 37.423290, mean_q: 75.697807
 15448/100000: episode: 144, duration: 0.321s, episode steps: 226, steps per second: 704, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.839861, mae: 37.730473, mean_q: 76.404808
 15655/100000: episode: 145, duration: 0.295s, episode steps: 207, steps per second: 702, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.246960, mae: 37.910362, mean_q: 76.705254
 15818/100000: episode: 146, duration: 0.236s, episode steps: 163, steps per second: 692, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.629910, mae: 37.607700, mean_q: 76.133911
 16037/100000: episode: 147, duration: 0.313s, episode steps: 219, steps per second: 701, episode reward: 219.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.773207, mae: 37.980042, mean_q: 76.882950
 16217/100000: episode: 148, duration: 0.269s, episode steps: 180, steps per second: 669, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.650855, mae: 38.281273, mean_q: 77.481720
 16387/100000: episode: 149, duration: 0.236s, episode steps: 170, steps per second: 719, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.227036, mae: 37.964664, mean_q: 76.825600
 16596/100000: episode: 150, duration: 0.289s, episode steps: 209, steps per second: 723, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.137023, mae: 38.174175, mean_q: 77.115265
 16769/100000: episode: 151, duration: 0.239s, episode steps: 173, steps per second: 725, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.354372, mae: 38.085224, mean_q: 76.882240
 16950/100000: episode: 152, duration: 0.251s, episode steps: 181, steps per second: 722, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.467677, mae: 37.907036, mean_q: 76.645317
 17170/100000: episode: 153, duration: 0.304s, episode steps: 220, steps per second: 724, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 1.811057, mae: 38.372143, mean_q: 77.565300
 17330/100000: episode: 154, duration: 0.222s, episode steps: 160, steps per second: 722, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.999660, mae: 38.786652, mean_q: 78.459816
 17561/100000: episode: 155, duration: 0.322s, episode steps: 231, steps per second: 718, episode reward: 231.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.754576, mae: 38.313709, mean_q: 77.590012
 17810/100000: episode: 156, duration: 0.367s, episode steps: 249, steps per second: 679, episode reward: 249.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.672741, mae: 38.519341, mean_q: 77.888870
 17994/100000: episode: 157, duration: 0.262s, episode steps: 184, steps per second: 703, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.236633, mae: 38.509327, mean_q: 77.765945
 18166/100000: episode: 158, duration: 0.252s, episode steps: 172, steps per second: 682, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.831066, mae: 38.482685, mean_q: 77.689011
 18339/100000: episode: 159, duration: 0.242s, episode steps: 173, steps per second: 715, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.780328, mae: 38.575233, mean_q: 77.849831
 18495/100000: episode: 160, duration: 0.220s, episode steps: 156, steps per second: 708, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.921986, mae: 38.682503, mean_q: 78.203041
 18703/100000: episode: 161, duration: 0.305s, episode steps: 208, steps per second: 683, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.779188, mae: 38.539654, mean_q: 77.835007
 18878/100000: episode: 162, duration: 0.249s, episode steps: 175, steps per second: 702, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.180642, mae: 39.175995, mean_q: 79.146118
 19056/100000: episode: 163, duration: 0.255s, episode steps: 178, steps per second: 697, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.051813, mae: 38.850861, mean_q: 78.413559
 19236/100000: episode: 164, duration: 0.257s, episode steps: 180, steps per second: 701, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.903812, mae: 39.051228, mean_q: 78.879852
 19383/100000: episode: 165, duration: 0.215s, episode steps: 147, steps per second: 683, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.224788, mae: 38.559731, mean_q: 77.794586
 19628/100000: episode: 166, duration: 0.366s, episode steps: 245, steps per second: 670, episode reward: 245.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 2.636229, mae: 38.894341, mean_q: 78.499352
 19830/100000: episode: 167, duration: 0.285s, episode steps: 202, steps per second: 710, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.066220, mae: 38.778561, mean_q: 78.207565
 20027/100000: episode: 168, duration: 0.274s, episode steps: 197, steps per second: 718, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.680660, mae: 38.671120, mean_q: 78.188080
 20218/100000: episode: 169, duration: 0.266s, episode steps: 191, steps per second: 718, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.576058, mae: 38.502567, mean_q: 77.746956
 20399/100000: episode: 170, duration: 0.254s, episode steps: 181, steps per second: 713, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.707309, mae: 38.673103, mean_q: 78.062531
 20606/100000: episode: 171, duration: 0.300s, episode steps: 207, steps per second: 689, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.546707, mae: 38.699963, mean_q: 78.177971
 20825/100000: episode: 172, duration: 0.310s, episode steps: 219, steps per second: 707, episode reward: 219.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.625991, mae: 38.880566, mean_q: 78.505447
 20992/100000: episode: 173, duration: 0.237s, episode steps: 167, steps per second: 704, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.844721, mae: 38.635368, mean_q: 77.852776
 21199/100000: episode: 174, duration: 0.299s, episode steps: 207, steps per second: 692, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.708793, mae: 38.358482, mean_q: 77.360893
 21454/100000: episode: 175, duration: 0.367s, episode steps: 255, steps per second: 694, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.838200, mae: 38.337807, mean_q: 77.442230
 21609/100000: episode: 176, duration: 0.218s, episode steps: 155, steps per second: 711, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.429784, mae: 38.457485, mean_q: 77.643272
 21784/100000: episode: 177, duration: 0.245s, episode steps: 175, steps per second: 714, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.915566, mae: 38.599068, mean_q: 77.945625
 21959/100000: episode: 178, duration: 0.248s, episode steps: 175, steps per second: 705, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.849331, mae: 38.602375, mean_q: 77.934921
 22173/100000: episode: 179, duration: 0.311s, episode steps: 214, steps per second: 687, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.056328, mae: 38.367039, mean_q: 77.465248
 22335/100000: episode: 180, duration: 0.236s, episode steps: 162, steps per second: 688, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.187997, mae: 38.664719, mean_q: 78.140045
 22509/100000: episode: 181, duration: 0.250s, episode steps: 174, steps per second: 697, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 2.407908, mae: 38.592285, mean_q: 77.819954
 22667/100000: episode: 182, duration: 0.230s, episode steps: 158, steps per second: 688, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 1.932703, mae: 38.156357, mean_q: 76.967751
 22828/100000: episode: 183, duration: 0.234s, episode steps: 161, steps per second: 689, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.242232, mae: 38.245491, mean_q: 77.219398
 23006/100000: episode: 184, duration: 0.256s, episode steps: 178, steps per second: 696, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.420519, mae: 38.469547, mean_q: 77.621300
 23373/100000: episode: 185, duration: 0.547s, episode steps: 367, steps per second: 670, episode reward: 367.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.383448, mae: 38.407043, mean_q: 77.504128
 23532/100000: episode: 186, duration: 0.247s, episode steps: 159, steps per second: 644, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.976280, mae: 38.547329, mean_q: 77.729256
 23709/100000: episode: 187, duration: 0.251s, episode steps: 177, steps per second: 706, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.981946, mae: 38.759830, mean_q: 78.197197
 23917/100000: episode: 188, duration: 0.297s, episode steps: 208, steps per second: 701, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.406468, mae: 38.132320, mean_q: 76.960014
 24100/100000: episode: 189, duration: 0.265s, episode steps: 183, steps per second: 691, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.757004, mae: 38.480873, mean_q: 77.665901
 24283/100000: episode: 190, duration: 0.259s, episode steps: 183, steps per second: 707, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.540436, mae: 38.466797, mean_q: 77.581322
 24523/100000: episode: 191, duration: 0.342s, episode steps: 240, steps per second: 701, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.020025, mae: 38.378181, mean_q: 77.395195
 24780/100000: episode: 192, duration: 0.373s, episode steps: 257, steps per second: 688, episode reward: 257.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.714891, mae: 38.580101, mean_q: 77.807114
 24964/100000: episode: 193, duration: 0.277s, episode steps: 184, steps per second: 665, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.656793, mae: 38.527836, mean_q: 77.721092
 25189/100000: episode: 194, duration: 0.334s, episode steps: 225, steps per second: 674, episode reward: 225.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.975308, mae: 38.441772, mean_q: 77.534630
 25426/100000: episode: 195, duration: 0.337s, episode steps: 237, steps per second: 702, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.910180, mae: 38.587166, mean_q: 77.826721
 25654/100000: episode: 196, duration: 0.333s, episode steps: 228, steps per second: 685, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 1.577194, mae: 38.688477, mean_q: 78.041443
 25892/100000: episode: 197, duration: 0.342s, episode steps: 238, steps per second: 696, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.755816, mae: 38.577568, mean_q: 77.801491
 26134/100000: episode: 198, duration: 0.351s, episode steps: 242, steps per second: 690, episode reward: 242.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.224797, mae: 38.673973, mean_q: 77.924957
 26339/100000: episode: 199, duration: 0.288s, episode steps: 205, steps per second: 711, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.258549, mae: 38.670925, mean_q: 77.927078
 26560/100000: episode: 200, duration: 0.311s, episode steps: 221, steps per second: 710, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.852641, mae: 38.837368, mean_q: 78.348473
 26775/100000: episode: 201, duration: 0.310s, episode steps: 215, steps per second: 694, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.053995, mae: 39.466862, mean_q: 79.633118
 26939/100000: episode: 202, duration: 0.244s, episode steps: 164, steps per second: 673, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 2.231316, mae: 38.975079, mean_q: 78.634354
 27106/100000: episode: 203, duration: 0.249s, episode steps: 167, steps per second: 672, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.837743, mae: 39.043945, mean_q: 78.598503
 27280/100000: episode: 204, duration: 0.254s, episode steps: 174, steps per second: 686, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.433417, mae: 39.053066, mean_q: 78.671356
 27699/100000: episode: 205, duration: 0.601s, episode steps: 419, steps per second: 697, episode reward: 419.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.988674, mae: 39.191971, mean_q: 79.039703
 27929/100000: episode: 206, duration: 0.327s, episode steps: 230, steps per second: 704, episode reward: 230.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.891620, mae: 39.494743, mean_q: 79.613274
 28152/100000: episode: 207, duration: 0.326s, episode steps: 223, steps per second: 685, episode reward: 223.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.502807, mae: 39.521809, mean_q: 79.644081
 28348/100000: episode: 208, duration: 0.280s, episode steps: 196, steps per second: 701, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.047827, mae: 39.763081, mean_q: 80.032578
 28556/100000: episode: 209, duration: 0.294s, episode steps: 208, steps per second: 708, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.804230, mae: 39.704998, mean_q: 80.068352
 28742/100000: episode: 210, duration: 0.270s, episode steps: 186, steps per second: 689, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.121639, mae: 39.931232, mean_q: 80.479698
 28938/100000: episode: 211, duration: 0.285s, episode steps: 196, steps per second: 687, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.390307, mae: 39.778778, mean_q: 80.197075
 29139/100000: episode: 212, duration: 0.294s, episode steps: 201, steps per second: 683, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.243955, mae: 39.884762, mean_q: 80.389603
 29362/100000: episode: 213, duration: 0.321s, episode steps: 223, steps per second: 696, episode reward: 223.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.596607, mae: 39.886509, mean_q: 80.377487
 29591/100000: episode: 214, duration: 0.332s, episode steps: 229, steps per second: 690, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.341099, mae: 39.970978, mean_q: 80.564819
 29872/100000: episode: 215, duration: 0.415s, episode steps: 281, steps per second: 678, episode reward: 281.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.111305, mae: 40.381248, mean_q: 81.303009
 30069/100000: episode: 216, duration: 0.291s, episode steps: 197, steps per second: 678, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 2.734394, mae: 40.304253, mean_q: 81.211205
 30433/100000: episode: 217, duration: 0.526s, episode steps: 364, steps per second: 693, episode reward: 364.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.823207, mae: 40.647980, mean_q: 82.043533
 30694/100000: episode: 218, duration: 0.375s, episode steps: 261, steps per second: 695, episode reward: 261.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.152023, mae: 40.606140, mean_q: 81.910370
 30988/100000: episode: 219, duration: 0.417s, episode steps: 294, steps per second: 705, episode reward: 294.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.305929, mae: 40.618153, mean_q: 81.831619
 31341/100000: episode: 220, duration: 0.507s, episode steps: 353, steps per second: 696, episode reward: 353.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.918927, mae: 40.859158, mean_q: 82.296844
 31573/100000: episode: 221, duration: 0.340s, episode steps: 232, steps per second: 682, episode reward: 232.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.118203, mae: 41.333202, mean_q: 83.316299
 31852/100000: episode: 222, duration: 0.409s, episode steps: 279, steps per second: 683, episode reward: 279.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.219906, mae: 41.184254, mean_q: 83.092255
 32094/100000: episode: 223, duration: 0.372s, episode steps: 242, steps per second: 650, episode reward: 242.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.850782, mae: 41.112686, mean_q: 82.876099
 32370/100000: episode: 224, duration: 0.417s, episode steps: 276, steps per second: 663, episode reward: 276.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.920167, mae: 41.515434, mean_q: 83.687485
 32557/100000: episode: 225, duration: 0.303s, episode steps: 187, steps per second: 617, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 2.728951, mae: 41.720181, mean_q: 84.144753
 32767/100000: episode: 226, duration: 0.312s, episode steps: 210, steps per second: 673, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.276089, mae: 41.516903, mean_q: 83.776863
 33022/100000: episode: 227, duration: 0.375s, episode steps: 255, steps per second: 681, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.891688, mae: 41.794052, mean_q: 84.277695
 33273/100000: episode: 228, duration: 0.361s, episode steps: 251, steps per second: 696, episode reward: 251.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.864882, mae: 41.537300, mean_q: 83.819527
 33551/100000: episode: 229, duration: 0.409s, episode steps: 278, steps per second: 680, episode reward: 278.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.369718, mae: 41.825794, mean_q: 84.352432
 33765/100000: episode: 230, duration: 0.332s, episode steps: 214, steps per second: 644, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.460189, mae: 41.708782, mean_q: 83.980431
 33974/100000: episode: 231, duration: 0.320s, episode steps: 209, steps per second: 654, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 2.148281, mae: 41.840073, mean_q: 84.216812
 34169/100000: episode: 232, duration: 0.292s, episode steps: 195, steps per second: 668, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 2.003668, mae: 41.598351, mean_q: 83.746140
 34429/100000: episode: 233, duration: 0.379s, episode steps: 260, steps per second: 687, episode reward: 260.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.254454, mae: 41.733089, mean_q: 84.154610
 34661/100000: episode: 234, duration: 0.344s, episode steps: 232, steps per second: 674, episode reward: 232.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.503196, mae: 41.758999, mean_q: 84.079323
 34917/100000: episode: 235, duration: 0.370s, episode steps: 256, steps per second: 692, episode reward: 256.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.937589, mae: 41.508430, mean_q: 83.537109
 35113/100000: episode: 236, duration: 0.292s, episode steps: 196, steps per second: 672, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.861771, mae: 41.614830, mean_q: 83.692574
 35343/100000: episode: 237, duration: 0.358s, episode steps: 230, steps per second: 643, episode reward: 230.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.009218, mae: 41.847996, mean_q: 84.235374
 35512/100000: episode: 238, duration: 0.250s, episode steps: 169, steps per second: 676, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.750735, mae: 41.605270, mean_q: 83.705521
 35677/100000: episode: 239, duration: 0.249s, episode steps: 165, steps per second: 663, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.149873, mae: 41.596050, mean_q: 83.860893
 35866/100000: episode: 240, duration: 0.284s, episode steps: 189, steps per second: 666, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 1.265486, mae: 41.785191, mean_q: 84.143311
 36056/100000: episode: 241, duration: 0.277s, episode steps: 190, steps per second: 687, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 1.547682, mae: 42.135635, mean_q: 84.705040
 36262/100000: episode: 242, duration: 0.300s, episode steps: 206, steps per second: 688, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.619336, mae: 41.545944, mean_q: 83.473335
 36443/100000: episode: 243, duration: 0.263s, episode steps: 181, steps per second: 689, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 2.513069, mae: 41.592957, mean_q: 83.491203
 36632/100000: episode: 244, duration: 0.276s, episode steps: 189, steps per second: 686, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.678555, mae: 41.385170, mean_q: 83.208580
 36880/100000: episode: 245, duration: 0.363s, episode steps: 248, steps per second: 683, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.874538, mae: 41.774292, mean_q: 83.912636
 37064/100000: episode: 246, duration: 0.277s, episode steps: 184, steps per second: 663, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.289652, mae: 41.933170, mean_q: 84.239326
 37277/100000: episode: 247, duration: 0.318s, episode steps: 213, steps per second: 669, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.908149, mae: 41.861404, mean_q: 84.164825
 37469/100000: episode: 248, duration: 0.295s, episode steps: 192, steps per second: 651, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 1.899760, mae: 41.960175, mean_q: 84.466805
 37637/100000: episode: 249, duration: 0.269s, episode steps: 168, steps per second: 624, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 2.065994, mae: 41.690094, mean_q: 83.868690
 37827/100000: episode: 250, duration: 0.283s, episode steps: 190, steps per second: 672, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.113017, mae: 41.788273, mean_q: 84.081062
 38044/100000: episode: 251, duration: 0.319s, episode steps: 217, steps per second: 680, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 1.304214, mae: 41.774708, mean_q: 83.987038
 38247/100000: episode: 252, duration: 0.305s, episode steps: 203, steps per second: 666, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.981884, mae: 41.494293, mean_q: 83.429016
 38431/100000: episode: 253, duration: 0.280s, episode steps: 184, steps per second: 656, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 1.995061, mae: 41.673965, mean_q: 83.653000
 38643/100000: episode: 254, duration: 0.322s, episode steps: 212, steps per second: 658, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.809589, mae: 41.793705, mean_q: 84.139908
 38915/100000: episode: 255, duration: 0.408s, episode steps: 272, steps per second: 667, episode reward: 272.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.935640, mae: 41.825569, mean_q: 84.126640
 39099/100000: episode: 256, duration: 0.286s, episode steps: 184, steps per second: 643, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.380095, mae: 41.944477, mean_q: 84.304253
 39292/100000: episode: 257, duration: 0.291s, episode steps: 193, steps per second: 663, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.696350, mae: 41.574944, mean_q: 83.446007
 39512/100000: episode: 258, duration: 0.329s, episode steps: 220, steps per second: 668, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.464722, mae: 41.495270, mean_q: 83.451668
 39791/100000: episode: 259, duration: 0.410s, episode steps: 279, steps per second: 681, episode reward: 279.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 2.294688, mae: 41.664291, mean_q: 83.746025
 39929/100000: episode: 260, duration: 0.207s, episode steps: 138, steps per second: 668, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.297381, mae: 41.741623, mean_q: 83.806908
 40150/100000: episode: 261, duration: 0.337s, episode steps: 221, steps per second: 655, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.749106, mae: 41.731564, mean_q: 83.746407
 40362/100000: episode: 262, duration: 0.316s, episode steps: 212, steps per second: 670, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.021101, mae: 41.625202, mean_q: 83.500237
 40611/100000: episode: 263, duration: 0.364s, episode steps: 249, steps per second: 683, episode reward: 249.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 1.676814, mae: 41.817192, mean_q: 84.051567
 40789/100000: episode: 264, duration: 0.264s, episode steps: 178, steps per second: 675, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 1.324237, mae: 42.020573, mean_q: 84.561325
 40974/100000: episode: 265, duration: 0.305s, episode steps: 185, steps per second: 606, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 2.179563, mae: 41.506062, mean_q: 83.425835
 41186/100000: episode: 266, duration: 0.320s, episode steps: 212, steps per second: 662, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.358484, mae: 41.712387, mean_q: 83.861809
 41437/100000: episode: 267, duration: 0.368s, episode steps: 251, steps per second: 682, episode reward: 251.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.080243, mae: 41.672932, mean_q: 83.768715
 41683/100000: episode: 268, duration: 0.378s, episode steps: 246, steps per second: 651, episode reward: 246.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.365355, mae: 41.920738, mean_q: 84.073486
 41885/100000: episode: 269, duration: 0.307s, episode steps: 202, steps per second: 659, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.893700, mae: 41.700966, mean_q: 83.845871
 42068/100000: episode: 270, duration: 0.279s, episode steps: 183, steps per second: 656, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.443 [0.000, 1.000],  loss: 1.870225, mae: 41.787178, mean_q: 84.111374
 42251/100000: episode: 271, duration: 0.274s, episode steps: 183, steps per second: 668, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 1.549055, mae: 42.028095, mean_q: 84.502136
 42453/100000: episode: 272, duration: 0.313s, episode steps: 202, steps per second: 646, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.589046, mae: 41.797127, mean_q: 84.094116
 42708/100000: episode: 273, duration: 0.386s, episode steps: 255, steps per second: 660, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.839411, mae: 41.928593, mean_q: 84.361198
 42909/100000: episode: 274, duration: 0.309s, episode steps: 201, steps per second: 651, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 1.032065, mae: 42.200851, mean_q: 84.923019
 43100/100000: episode: 275, duration: 0.307s, episode steps: 191, steps per second: 622, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.043007, mae: 42.023441, mean_q: 84.398224
 43246/100000: episode: 276, duration: 0.218s, episode steps: 146, steps per second: 670, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.699860, mae: 41.901463, mean_q: 84.233879
 43454/100000: episode: 277, duration: 0.308s, episode steps: 208, steps per second: 675, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 2.282866, mae: 41.805290, mean_q: 83.974945
 43667/100000: episode: 278, duration: 0.320s, episode steps: 213, steps per second: 666, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 1.702685, mae: 41.796467, mean_q: 84.063049
 43855/100000: episode: 279, duration: 0.288s, episode steps: 188, steps per second: 654, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 1.389926, mae: 41.708263, mean_q: 83.819107
 44012/100000: episode: 280, duration: 0.245s, episode steps: 157, steps per second: 642, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 1.479297, mae: 41.972244, mean_q: 84.319489
 44210/100000: episode: 281, duration: 0.307s, episode steps: 198, steps per second: 645, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.444888, mae: 41.895664, mean_q: 84.237915
 44471/100000: episode: 282, duration: 0.399s, episode steps: 261, steps per second: 653, episode reward: 261.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.864773, mae: 42.145565, mean_q: 84.767708
 44673/100000: episode: 283, duration: 0.303s, episode steps: 202, steps per second: 667, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.731594, mae: 41.662273, mean_q: 83.751259
 44927/100000: episode: 284, duration: 0.381s, episode steps: 254, steps per second: 666, episode reward: 254.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 2.131725, mae: 42.091412, mean_q: 84.592194
 45198/100000: episode: 285, duration: 0.406s, episode steps: 271, steps per second: 667, episode reward: 271.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 1.135088, mae: 41.880287, mean_q: 84.239975
 45384/100000: episode: 286, duration: 0.277s, episode steps: 186, steps per second: 670, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 1.592099, mae: 42.482334, mean_q: 85.377373
 45579/100000: episode: 287, duration: 0.291s, episode steps: 195, steps per second: 671, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 2.519292, mae: 41.961620, mean_q: 84.397804
 45777/100000: episode: 288, duration: 0.296s, episode steps: 198, steps per second: 669, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.739716, mae: 42.040611, mean_q: 84.532585
 46015/100000: episode: 289, duration: 0.355s, episode steps: 238, steps per second: 671, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.327932, mae: 42.276234, mean_q: 85.120865
 46200/100000: episode: 290, duration: 0.276s, episode steps: 185, steps per second: 671, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 1.556080, mae: 41.901173, mean_q: 84.366699
 46393/100000: episode: 291, duration: 0.288s, episode steps: 193, steps per second: 669, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 1.262945, mae: 42.222557, mean_q: 85.135086
 46613/100000: episode: 292, duration: 0.329s, episode steps: 220, steps per second: 668, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 1.414308, mae: 42.272930, mean_q: 85.075035
 46784/100000: episode: 293, duration: 0.256s, episode steps: 171, steps per second: 668, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 0.886936, mae: 42.317669, mean_q: 85.278519
 46975/100000: episode: 294, duration: 0.285s, episode steps: 191, steps per second: 670, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.200775, mae: 41.985413, mean_q: 84.342888
 47213/100000: episode: 295, duration: 0.355s, episode steps: 238, steps per second: 671, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 1.182320, mae: 42.224213, mean_q: 84.976807
 47405/100000: episode: 296, duration: 0.289s, episode steps: 192, steps per second: 664, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 1.998674, mae: 42.338123, mean_q: 85.234047
 47584/100000: episode: 297, duration: 0.268s, episode steps: 179, steps per second: 667, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 1.835051, mae: 41.799229, mean_q: 84.061897
 47782/100000: episode: 298, duration: 0.295s, episode steps: 198, steps per second: 670, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 2.363482, mae: 41.967300, mean_q: 84.402603
 47978/100000: episode: 299, duration: 0.293s, episode steps: 196, steps per second: 668, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 1.061441, mae: 41.763832, mean_q: 84.113182
 48169/100000: episode: 300, duration: 0.285s, episode steps: 191, steps per second: 669, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 0.765453, mae: 42.242107, mean_q: 85.040649
 48355/100000: episode: 301, duration: 0.279s, episode steps: 186, steps per second: 666, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 1.728544, mae: 42.221340, mean_q: 84.971115
 48539/100000: episode: 302, duration: 0.276s, episode steps: 184, steps per second: 666, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.025427, mae: 41.956158, mean_q: 84.481071
 48708/100000: episode: 303, duration: 0.254s, episode steps: 169, steps per second: 666, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.840744, mae: 41.216286, mean_q: 82.866005
 48863/100000: episode: 304, duration: 0.233s, episode steps: 155, steps per second: 666, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 2.403600, mae: 41.372913, mean_q: 83.050514
 49040/100000: episode: 305, duration: 0.267s, episode steps: 177, steps per second: 662, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 2.153093, mae: 41.543217, mean_q: 83.526619
 49196/100000: episode: 306, duration: 0.235s, episode steps: 156, steps per second: 664, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.570365, mae: 41.606369, mean_q: 83.640465
 49382/100000: episode: 307, duration: 0.281s, episode steps: 186, steps per second: 662, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.682486, mae: 41.960835, mean_q: 84.406448
 49573/100000: episode: 308, duration: 0.287s, episode steps: 191, steps per second: 666, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 1.621222, mae: 41.915104, mean_q: 84.285942
 49764/100000: episode: 309, duration: 0.288s, episode steps: 191, steps per second: 663, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.044670, mae: 41.931461, mean_q: 84.499992
 49933/100000: episode: 310, duration: 0.254s, episode steps: 169, steps per second: 665, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.591851, mae: 41.771683, mean_q: 84.057999
 50129/100000: episode: 311, duration: 0.295s, episode steps: 196, steps per second: 663, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 1.673723, mae: 41.828632, mean_q: 84.091019
 50292/100000: episode: 312, duration: 0.246s, episode steps: 163, steps per second: 663, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 1.254773, mae: 41.332939, mean_q: 83.236664
 50512/100000: episode: 313, duration: 0.330s, episode steps: 220, steps per second: 667, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 2.015362, mae: 41.407181, mean_q: 83.252151
 50718/100000: episode: 314, duration: 0.310s, episode steps: 206, steps per second: 664, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.977333, mae: 41.704746, mean_q: 83.846275
 50993/100000: episode: 315, duration: 0.415s, episode steps: 275, steps per second: 662, episode reward: 275.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.580865, mae: 41.440971, mean_q: 83.330811
 51162/100000: episode: 316, duration: 0.255s, episode steps: 169, steps per second: 664, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.644872, mae: 41.301640, mean_q: 83.091187
 51346/100000: episode: 317, duration: 0.277s, episode steps: 184, steps per second: 663, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 1.034998, mae: 41.379513, mean_q: 83.153351
 51569/100000: episode: 318, duration: 0.335s, episode steps: 223, steps per second: 666, episode reward: 223.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.085937, mae: 41.295319, mean_q: 82.930649
 51821/100000: episode: 319, duration: 0.379s, episode steps: 252, steps per second: 665, episode reward: 252.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.869857, mae: 41.142834, mean_q: 82.652794
 52050/100000: episode: 320, duration: 0.344s, episode steps: 229, steps per second: 666, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.915816, mae: 40.882439, mean_q: 82.138527
 52256/100000: episode: 321, duration: 0.309s, episode steps: 206, steps per second: 667, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.090577, mae: 41.202152, mean_q: 82.691200
 52600/100000: episode: 322, duration: 0.516s, episode steps: 344, steps per second: 667, episode reward: 344.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.770663, mae: 40.834934, mean_q: 81.960213
 52857/100000: episode: 323, duration: 0.385s, episode steps: 257, steps per second: 668, episode reward: 257.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.386421, mae: 40.532299, mean_q: 81.459602
 53081/100000: episode: 324, duration: 0.336s, episode steps: 224, steps per second: 667, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.732089, mae: 40.475361, mean_q: 81.211388
 53287/100000: episode: 325, duration: 0.310s, episode steps: 206, steps per second: 665, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.362766, mae: 40.730690, mean_q: 81.810013
 53496/100000: episode: 326, duration: 0.314s, episode steps: 209, steps per second: 665, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.432624, mae: 40.591385, mean_q: 81.486214
 53775/100000: episode: 327, duration: 0.418s, episode steps: 279, steps per second: 667, episode reward: 279.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 0.497683, mae: 40.264114, mean_q: 80.792480
 54025/100000: episode: 328, duration: 0.374s, episode steps: 250, steps per second: 668, episode reward: 250.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.407147, mae: 40.287086, mean_q: 80.809486
 54244/100000: episode: 329, duration: 0.328s, episode steps: 219, steps per second: 669, episode reward: 219.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 0.383189, mae: 39.962856, mean_q: 80.200630
 54473/100000: episode: 330, duration: 0.345s, episode steps: 229, steps per second: 664, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.345553, mae: 40.167713, mean_q: 80.599228
 54664/100000: episode: 331, duration: 0.287s, episode steps: 191, steps per second: 666, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 0.354845, mae: 40.259502, mean_q: 80.776962
 54894/100000: episode: 332, duration: 0.343s, episode steps: 230, steps per second: 670, episode reward: 230.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 0.340738, mae: 39.764675, mean_q: 79.818260
 55140/100000: episode: 333, duration: 0.368s, episode steps: 246, steps per second: 668, episode reward: 246.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.256061, mae: 39.565239, mean_q: 79.392273
 55345/100000: episode: 334, duration: 0.307s, episode steps: 205, steps per second: 668, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 0.280800, mae: 39.293087, mean_q: 78.836388
 55606/100000: episode: 335, duration: 0.390s, episode steps: 261, steps per second: 669, episode reward: 261.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 0.304487, mae: 39.543568, mean_q: 79.277725
 56049/100000: episode: 336, duration: 0.660s, episode steps: 443, steps per second: 671, episode reward: 443.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.291573, mae: 39.334007, mean_q: 78.853531
 56282/100000: episode: 337, duration: 0.348s, episode steps: 233, steps per second: 670, episode reward: 233.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.393720, mae: 39.691086, mean_q: 79.508720
 56763/100000: episode: 338, duration: 0.718s, episode steps: 481, steps per second: 670, episode reward: 481.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.323388, mae: 39.168232, mean_q: 78.459305
 57263/100000: episode: 339, duration: 0.744s, episode steps: 500, steps per second: 672, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.281920, mae: 39.389736, mean_q: 78.957947
 57636/100000: episode: 340, duration: 0.559s, episode steps: 373, steps per second: 668, episode reward: 373.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.344636, mae: 39.479366, mean_q: 79.120033
 57876/100000: episode: 341, duration: 0.359s, episode steps: 240, steps per second: 668, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 0.300314, mae: 39.542130, mean_q: 79.294868
 58073/100000: episode: 342, duration: 0.299s, episode steps: 197, steps per second: 659, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.226796, mae: 39.068794, mean_q: 78.443352
 58490/100000: episode: 343, duration: 0.623s, episode steps: 417, steps per second: 669, episode reward: 417.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 0.223351, mae: 39.187088, mean_q: 78.675438
 58660/100000: episode: 344, duration: 0.255s, episode steps: 170, steps per second: 666, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.249732, mae: 39.227234, mean_q: 78.714340
 59160/100000: episode: 345, duration: 0.745s, episode steps: 500, steps per second: 671, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.226484, mae: 39.315601, mean_q: 78.945450
 59660/100000: episode: 346, duration: 0.755s, episode steps: 500, steps per second: 662, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.228457, mae: 39.265671, mean_q: 78.848816
 60108/100000: episode: 347, duration: 0.668s, episode steps: 448, steps per second: 670, episode reward: 448.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.104419, mae: 39.472378, mean_q: 79.220947
 60521/100000: episode: 348, duration: 0.618s, episode steps: 413, steps per second: 669, episode reward: 413.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.499 [0.000, 1.000],  loss: 0.255554, mae: 39.721771, mean_q: 79.756973
 60774/100000: episode: 349, duration: 0.379s, episode steps: 253, steps per second: 667, episode reward: 253.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.643551, mae: 39.890446, mean_q: 79.995895
 61274/100000: episode: 350, duration: 0.758s, episode steps: 500, steps per second: 660, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 0.835735, mae: 39.493347, mean_q: 79.202591
 61774/100000: episode: 351, duration: 0.765s, episode steps: 500, steps per second: 654, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.260764, mae: 39.613377, mean_q: 79.523811
 62274/100000: episode: 352, duration: 0.754s, episode steps: 500, steps per second: 663, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.482027, mae: 40.086781, mean_q: 80.384453
 62774/100000: episode: 353, duration: 0.758s, episode steps: 500, steps per second: 659, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.717618, mae: 40.095886, mean_q: 80.473747
 63274/100000: episode: 354, duration: 0.749s, episode steps: 500, steps per second: 668, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.991921, mae: 40.544151, mean_q: 81.426781
 63774/100000: episode: 355, duration: 0.766s, episode steps: 500, steps per second: 653, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.741296, mae: 40.748638, mean_q: 81.805130
 64274/100000: episode: 356, duration: 0.749s, episode steps: 500, steps per second: 668, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.682427, mae: 41.005272, mean_q: 82.351524
 64774/100000: episode: 357, duration: 0.748s, episode steps: 500, steps per second: 669, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 1.376782, mae: 41.666016, mean_q: 83.633842
 65274/100000: episode: 358, duration: 0.754s, episode steps: 500, steps per second: 663, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.565175, mae: 41.905014, mean_q: 84.185722
 65774/100000: episode: 359, duration: 0.754s, episode steps: 500, steps per second: 663, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.744095, mae: 42.495201, mean_q: 85.428589
 66274/100000: episode: 360, duration: 0.776s, episode steps: 500, steps per second: 644, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 1.931705, mae: 42.912849, mean_q: 86.203323
 66774/100000: episode: 361, duration: 0.754s, episode steps: 500, steps per second: 663, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.918455, mae: 43.377476, mean_q: 87.065544
 67274/100000: episode: 362, duration: 0.777s, episode steps: 500, steps per second: 643, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.330976, mae: 43.802612, mean_q: 87.857483
 67774/100000: episode: 363, duration: 0.773s, episode steps: 500, steps per second: 646, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 2.850009, mae: 43.991772, mean_q: 88.242065
 68274/100000: episode: 364, duration: 0.779s, episode steps: 500, steps per second: 642, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.989881, mae: 44.333172, mean_q: 88.950256
 68527/100000: episode: 365, duration: 0.409s, episode steps: 253, steps per second: 619, episode reward: 253.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.320850, mae: 44.174217, mean_q: 88.571991
 69027/100000: episode: 366, duration: 0.776s, episode steps: 500, steps per second: 644, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 1.751418, mae: 44.739761, mean_q: 89.731033
 69527/100000: episode: 367, duration: 0.757s, episode steps: 500, steps per second: 660, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.968558, mae: 44.835056, mean_q: 89.865891
 70027/100000: episode: 368, duration: 0.783s, episode steps: 500, steps per second: 639, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 3.283084, mae: 44.862110, mean_q: 89.909721
 70527/100000: episode: 369, duration: 0.762s, episode steps: 500, steps per second: 656, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 2.638283, mae: 44.972092, mean_q: 90.143295
 71027/100000: episode: 370, duration: 0.766s, episode steps: 500, steps per second: 653, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.223250, mae: 45.225773, mean_q: 90.684113
 71122/100000: episode: 371, duration: 0.146s, episode steps:  95, steps per second: 650, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.245487, mae: 45.192696, mean_q: 90.463692
 71583/100000: episode: 372, duration: 0.713s, episode steps: 461, steps per second: 647, episode reward: 461.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 3.996353, mae: 45.512787, mean_q: 91.191071
 72083/100000: episode: 373, duration: 0.762s, episode steps: 500, steps per second: 656, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.005315, mae: 45.353180, mean_q: 90.850487
 72583/100000: episode: 374, duration: 0.749s, episode steps: 500, steps per second: 667, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.837138, mae: 45.702141, mean_q: 91.583923
 72983/100000: episode: 375, duration: 0.601s, episode steps: 400, steps per second: 666, episode reward: 400.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.151417, mae: 46.243614, mean_q: 92.778519
 73483/100000: episode: 376, duration: 0.773s, episode steps: 500, steps per second: 647, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 2.855067, mae: 46.067329, mean_q: 92.376152
 73762/100000: episode: 377, duration: 0.433s, episode steps: 279, steps per second: 644, episode reward: 279.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 7.306074, mae: 46.572357, mean_q: 93.165520
 74058/100000: episode: 378, duration: 0.475s, episode steps: 296, steps per second: 624, episode reward: 296.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.523611, mae: 46.468983, mean_q: 93.247704
 74494/100000: episode: 379, duration: 0.654s, episode steps: 436, steps per second: 666, episode reward: 436.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 3.532272, mae: 46.496460, mean_q: 93.197563
 74749/100000: episode: 380, duration: 0.382s, episode steps: 255, steps per second: 667, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.480887, mae: 46.315037, mean_q: 92.753044
 75113/100000: episode: 381, duration: 0.543s, episode steps: 364, steps per second: 670, episode reward: 364.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 3.034751, mae: 46.468933, mean_q: 93.212227
 75613/100000: episode: 382, duration: 0.748s, episode steps: 500, steps per second: 669, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 2.371769, mae: 46.985245, mean_q: 94.249245
 76113/100000: episode: 383, duration: 0.760s, episode steps: 500, steps per second: 658, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 2.826310, mae: 47.088474, mean_q: 94.413406
 76448/100000: episode: 384, duration: 0.516s, episode steps: 335, steps per second: 649, episode reward: 335.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.662513, mae: 47.551266, mean_q: 95.372368
 76888/100000: episode: 385, duration: 0.675s, episode steps: 440, steps per second: 652, episode reward: 440.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.700483, mae: 47.368572, mean_q: 94.976570
 77388/100000: episode: 386, duration: 0.810s, episode steps: 500, steps per second: 617, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 2.596115, mae: 47.526508, mean_q: 95.289825
 77888/100000: episode: 387, duration: 0.776s, episode steps: 500, steps per second: 644, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.095445, mae: 47.829964, mean_q: 95.857109
 78108/100000: episode: 388, duration: 0.332s, episode steps: 220, steps per second: 663, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.429566, mae: 48.034237, mean_q: 96.422386
 78608/100000: episode: 389, duration: 0.747s, episode steps: 500, steps per second: 670, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 2.832143, mae: 48.244835, mean_q: 96.726273
 79108/100000: episode: 390, duration: 0.752s, episode steps: 500, steps per second: 665, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 3.173866, mae: 48.341133, mean_q: 96.865356
 79608/100000: episode: 391, duration: 0.766s, episode steps: 500, steps per second: 652, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 2.729353, mae: 48.480469, mean_q: 97.209312
 80108/100000: episode: 392, duration: 0.767s, episode steps: 500, steps per second: 652, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.200381, mae: 48.675812, mean_q: 97.497406
 80608/100000: episode: 393, duration: 0.770s, episode steps: 500, steps per second: 650, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.117543, mae: 48.733711, mean_q: 97.568504
 80963/100000: episode: 394, duration: 0.545s, episode steps: 355, steps per second: 651, episode reward: 355.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 4.278563, mae: 48.745686, mean_q: 97.616318
 81463/100000: episode: 395, duration: 0.763s, episode steps: 500, steps per second: 655, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.873819, mae: 48.640610, mean_q: 97.420998
 81760/100000: episode: 396, duration: 0.450s, episode steps: 297, steps per second: 660, episode reward: 297.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.026205, mae: 48.429344, mean_q: 97.015358
 82260/100000: episode: 397, duration: 0.760s, episode steps: 500, steps per second: 658, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 4.050301, mae: 48.405895, mean_q: 96.985146
 82577/100000: episode: 398, duration: 0.483s, episode steps: 317, steps per second: 656, episode reward: 317.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 6.239956, mae: 48.626602, mean_q: 97.319336
 82619/100000: episode: 399, duration: 0.065s, episode steps:  42, steps per second: 651, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 12.688499, mae: 48.388969, mean_q: 96.481544
 82882/100000: episode: 400, duration: 0.395s, episode steps: 263, steps per second: 665, episode reward: 263.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.081761, mae: 48.397945, mean_q: 96.980095
 83194/100000: episode: 401, duration: 0.478s, episode steps: 312, steps per second: 652, episode reward: 312.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.117612, mae: 48.405869, mean_q: 96.998138
 83268/100000: episode: 402, duration: 0.114s, episode steps:  74, steps per second: 646, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 10.035253, mae: 49.052361, mean_q: 98.110031
 83553/100000: episode: 403, duration: 0.435s, episode steps: 285, steps per second: 656, episode reward: 285.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 4.380404, mae: 48.380142, mean_q: 97.011719
 83747/100000: episode: 404, duration: 0.297s, episode steps: 194, steps per second: 653, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.285810, mae: 48.965248, mean_q: 98.114288
 84247/100000: episode: 405, duration: 0.752s, episode steps: 500, steps per second: 665, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.272666, mae: 48.921707, mean_q: 98.062836
 84446/100000: episode: 406, duration: 0.305s, episode steps: 199, steps per second: 651, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.259803, mae: 49.039299, mean_q: 98.438622
 84720/100000: episode: 407, duration: 0.421s, episode steps: 274, steps per second: 651, episode reward: 274.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.232781, mae: 48.834133, mean_q: 98.038635
 85220/100000: episode: 408, duration: 0.767s, episode steps: 500, steps per second: 652, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.755290, mae: 49.185329, mean_q: 98.679794
 85720/100000: episode: 409, duration: 0.767s, episode steps: 500, steps per second: 652, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 7.192128, mae: 49.519508, mean_q: 99.278816
 86220/100000: episode: 410, duration: 0.758s, episode steps: 500, steps per second: 660, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 7.284723, mae: 49.399025, mean_q: 99.018669
 86404/100000: episode: 411, duration: 0.280s, episode steps: 184, steps per second: 657, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.808694, mae: 49.622536, mean_q: 99.594215
 86904/100000: episode: 412, duration: 0.758s, episode steps: 500, steps per second: 659, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.012132, mae: 49.717449, mean_q: 99.777061
 87215/100000: episode: 413, duration: 0.468s, episode steps: 311, steps per second: 664, episode reward: 311.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 3.909634, mae: 50.018085, mean_q: 100.366676
 87491/100000: episode: 414, duration: 0.426s, episode steps: 276, steps per second: 648, episode reward: 276.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5.364490, mae: 50.175816, mean_q: 100.577827
 87991/100000: episode: 415, duration: 0.751s, episode steps: 500, steps per second: 666, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.481406, mae: 50.236122, mean_q: 100.737114
 88491/100000: episode: 416, duration: 0.748s, episode steps: 500, steps per second: 668, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.521898, mae: 50.748306, mean_q: 101.817253
 88991/100000: episode: 417, duration: 0.776s, episode steps: 500, steps per second: 645, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 8.101469, mae: 50.794266, mean_q: 101.830688
 89491/100000: episode: 418, duration: 0.755s, episode steps: 500, steps per second: 663, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 8.351657, mae: 51.092110, mean_q: 102.513748
 89991/100000: episode: 419, duration: 0.777s, episode steps: 500, steps per second: 643, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.008074, mae: 51.272976, mean_q: 102.802780
 90272/100000: episode: 420, duration: 0.419s, episode steps: 281, steps per second: 670, episode reward: 281.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 7.555603, mae: 51.348476, mean_q: 103.002853
 90772/100000: episode: 421, duration: 0.757s, episode steps: 500, steps per second: 661, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.406243, mae: 51.401283, mean_q: 103.014030
 91248/100000: episode: 422, duration: 0.731s, episode steps: 476, steps per second: 651, episode reward: 476.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 9.026939, mae: 51.563492, mean_q: 103.313713
 91748/100000: episode: 423, duration: 0.780s, episode steps: 500, steps per second: 641, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.918176, mae: 51.613976, mean_q: 103.501976
 92214/100000: episode: 424, duration: 0.716s, episode steps: 466, steps per second: 651, episode reward: 466.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 8.490263, mae: 51.656845, mean_q: 103.507576
 92714/100000: episode: 425, duration: 0.766s, episode steps: 500, steps per second: 653, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 6.782072, mae: 51.696049, mean_q: 103.670547
 93214/100000: episode: 426, duration: 0.778s, episode steps: 500, steps per second: 642, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.197476, mae: 51.871407, mean_q: 104.063606
 93714/100000: episode: 427, duration: 0.764s, episode steps: 500, steps per second: 655, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.697500, mae: 52.265011, mean_q: 104.756111
 94214/100000: episode: 428, duration: 0.770s, episode steps: 500, steps per second: 649, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 8.600530, mae: 52.101952, mean_q: 104.441429
 94714/100000: episode: 429, duration: 0.770s, episode steps: 500, steps per second: 650, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 6.538638, mae: 52.341061, mean_q: 105.009834
 95214/100000: episode: 430, duration: 0.748s, episode steps: 500, steps per second: 668, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 9.070166, mae: 52.241791, mean_q: 104.631187
 95268/100000: episode: 431, duration: 0.081s, episode steps:  54, steps per second: 663, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 8.478456, mae: 52.390739, mean_q: 104.925827
 95722/100000: episode: 432, duration: 0.678s, episode steps: 454, steps per second: 670, episode reward: 454.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 10.224427, mae: 52.148342, mean_q: 104.455154
 95962/100000: episode: 433, duration: 0.359s, episode steps: 240, steps per second: 669, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 7.388158, mae: 51.873276, mean_q: 104.011505
 96462/100000: episode: 434, duration: 0.748s, episode steps: 500, steps per second: 668, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 7.698998, mae: 51.747444, mean_q: 103.712471
 96737/100000: episode: 435, duration: 0.411s, episode steps: 275, steps per second: 669, episode reward: 275.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 5.595512, mae: 51.815689, mean_q: 103.946793
 96880/100000: episode: 436, duration: 0.214s, episode steps: 143, steps per second: 667, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.519220, mae: 51.707081, mean_q: 103.677734
 97353/100000: episode: 437, duration: 0.706s, episode steps: 473, steps per second: 670, episode reward: 473.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 6.636974, mae: 51.731060, mean_q: 103.738693
 97853/100000: episode: 438, duration: 0.746s, episode steps: 500, steps per second: 671, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.992207, mae: 51.550503, mean_q: 103.425140
 98353/100000: episode: 439, duration: 0.746s, episode steps: 500, steps per second: 671, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.294095, mae: 51.550686, mean_q: 103.312096
 98853/100000: episode: 440, duration: 0.747s, episode steps: 500, steps per second: 669, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 5.785152, mae: 51.501389, mean_q: 103.312325
 99353/100000: episode: 441, duration: 0.743s, episode steps: 500, steps per second: 673, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.057621, mae: 51.740784, mean_q: 103.614571
 99805/100000: episode: 442, duration: 0.677s, episode steps: 452, steps per second: 668, episode reward: 452.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 7.960107, mae: 51.884583, mean_q: 103.886833
done, took 148.706 seconds
Testing for 10 episodes ...
Episode 1: reward: 500.000, steps: 500
Episode 2: reward: 500.000, steps: 500
Episode 3: reward: 500.000, steps: 500
Episode 4: reward: 500.000, steps: 500
Episode 5: reward: 500.000, steps: 500
Episode 6: reward: 500.000, steps: 500
Episode 7: reward: 500.000, steps: 500
Episode 8: reward: 500.000, steps: 500
Episode 9: reward: 500.000, steps: 500
Episode 10: reward: 500.000, steps: 500
[500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]
